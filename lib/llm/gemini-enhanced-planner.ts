import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { GoogleGenerativeAI } from "@google/generative-ai";

// ==================== æ ¸å¿ƒæ¥å£å®šä¹‰ ====================
interface ShotPlan {
  ratio: string;
  total_seconds: number;
  shots: Array<{
    id: number;
    prompt: string;
    duration_s: number;
    camera: string;
  }>;
}

interface NarrativeAnalysis {
  genre: string;
  narrative_structure: 'linear' | 'flashback' | 'parallel' | 'circular';
  characters: CharacterProfile[];
  setting: SceneSetting;
  emotional_arc: EmotionalPoint[];
  key_themes: string[];
  visual_style: VisualStyleGuide;
  pacing_rhythm: PacingProfile;
  identity_anchors?: string; // ä»å‚è€ƒå›¾ç‰‡æå–çš„èº«ä»½é”šç‚¹ï¼ˆå¯é€‰ï¼‰
  subject_type?: 'person' | 'object'; // ä¸»ä½“ç±»å‹ï¼šäººæˆ–ç‰©ä½“
  subject_descriptor?: string; // ç”¨äº shot 1 å‰ç¼€çš„æè¿°è¯
}

interface CharacterProfile {
  id: string;
  name: string;
  physical_description: string;
  clothing_style: string;
  distinctive_features: string[];
  consistency_keywords: string[];
}

interface SceneSetting {
  time_period: string;
  location: string;
  atmosphere: string;
  lighting_conditions: string;
}

interface EmotionalPoint {
  timestamp: number;
  emotional_state: string;
  intensity: number;
  visual_cues: string;
}

interface VisualStyleGuide {
  color_scheme: string;
  cinematography_style: string;
  movement_style: string;
  artistic_reference: string;
}

interface PacingProfile {
  overall_tempo: 'slow' | 'medium' | 'fast';
  rhythm_changes: Array<{
    at_second: number;
    new_tempo: 'slow' | 'medium' | 'fast';
    reason: string;
  }>;
}

interface EnhancedScenePlan {
  id: string;
  sequence_number: number;
  duration_seconds: number;
  narrative_purpose: 'exposition' | 'rising_action' | 'climax' | 'falling_action' | 'resolution';
  visual_description: string;
  camera_movement: CameraMovement;
  lighting_style: LightingProfile;
  color_palette: ColorPalette;
  characters_present: string[];
  consistency_anchors: ConsistencyAnchor[];
  runway_prompt: string;
}

interface CameraMovement {
  type: string;
  speed: string;
  description: string;
}

interface LightingProfile {
  type: string;
  direction: string;
  mood: string;
}

interface ColorPalette {
  primary_colors: string[];
  secondary_colors: string[];
  mood_descriptor: string;
}

interface ConsistencyAnchor {
  type: 'character' | 'environment' | 'lighting' | 'color';
  reference_description: string;
  importance_weight: number;
  consistency_prompt: string;
}

interface ImageAnalysisResult {
  identityAnchors: string; // å®Œæ•´çš„èº«ä»½é”šç‚¹æè¿°
  subjectType: 'person' | 'object'; // ä¸»ä½“ç±»å‹
  subjectDescriptor: string; // ç®€çŸ­çš„ä¸»ä½“æè¿°è¯ï¼ˆå¦‚ "necklace", "the woman" ç­‰ï¼‰
}

// ==================== å¢å¼ºGeminiåˆ†æå™¨ ====================
class GeminiEnhancedShotPlanner {
  private gemini: ChatGoogleGenerativeAI;
  private genAI: GoogleGenerativeAI;
  private visionModel: any;

  constructor() {
    // æ£€æŸ¥API key
    const apiKey = process.env.GEMINI_API_KEY || process.env.GOOGLE_API_KEY;
    if (!apiKey) {
      throw new Error("GEMINI_API_KEY or GOOGLE_API_KEY is not set");
    }

    // é…ç½®Gemini 2.5 Flash - æœ€æ–°æœ€å¿«çš„ç‰ˆæœ¬
    this.gemini = new ChatGoogleGenerativeAI({
      model: "gemini-2.0-flash-exp", // ä½¿ç”¨å®éªŒç‰ˆè·å¾—æœ€æ–°åŠŸèƒ½
      temperature: 0.7,
      maxOutputTokens: 8192,
      topK: 40,
      topP: 0.95,
      apiKey: apiKey, // æ˜¾å¼ä¼ é€’API key
      // safetySettingså¯èƒ½åœ¨æŸäº›ç‰ˆæœ¬ä¸­æœ‰ç±»å‹é—®é¢˜ï¼Œæš‚æ—¶ç§»é™¤
      // safetySettings: [
      //   {
      //     category: "HARM_CATEGORY_HARASSMENT",
      //     threshold: "BLOCK_ONLY_HIGH"
      //   }
      // ]
    });

    // å¤šæ¨¡æ€åˆ†æå®¢æˆ·ç«¯
    this.genAI = new GoogleGenerativeAI(apiKey);
    this.visionModel = this.genAI.getGenerativeModel({
      model: "gemini-2.0-flash-exp",
      generationConfig: {
        temperature: 0.6,
        topK: 32,
        topP: 0.9,
        maxOutputTokens: 4096,
      }
    });
  }

  async analyzeUserInput(userInput: string, referenceImages?: string[]): Promise<NarrativeAnalysis> {
    console.log('ğŸ§  ä½¿ç”¨Gemini 2.5 Flashè¿›è¡Œæ·±åº¦å™äº‹åˆ†æ...');

    // æ­¥éª¤1ï¼šå¦‚æœç”¨æˆ·ä¸Šä¼ äº†å‚è€ƒå›¾ç‰‡ï¼Œæå–èº«ä»½é”šç‚¹å’Œä¸»ä½“ä¿¡æ¯
    let imageAnalysis: ImageAnalysisResult | null = null;
    if (referenceImages && referenceImages.length > 0) {
      console.log(`ğŸ–¼ï¸ æ£€æµ‹åˆ° ${referenceImages.length} å¼ å‚è€ƒå›¾ç‰‡ï¼Œæå–èº«ä»½é”šç‚¹...`);
      imageAnalysis = await this.analyzeReferenceImages(referenceImages);
    }

    // æ­¥éª¤2ï¼šè¿›è¡Œå™äº‹åˆ†æ
    const analysisPrompt = `
ä½ æ˜¯ä¸–ç•Œçº§çš„ç”µå½±å™äº‹åˆ†æä¸“å®¶ã€‚è¯·æ·±åº¦åˆ†æä»¥ä¸‹ç”¨æˆ·çš„è§†é¢‘ç”Ÿæˆéœ€æ±‚ï¼š

ç”¨æˆ·è¾“å…¥ï¼š"${userInput}"
${imageAnalysis ? `\n**å‚è€ƒå›¾ç‰‡èº«ä»½é”šç‚¹**ï¼š${imageAnalysis.identityAnchors}\n\nâš ï¸ é‡è¦ï¼šç”¨æˆ·ä¸Šä¼ äº†å‚è€ƒå›¾ç‰‡ã€‚åœ¨åˆ†æè§’è‰²/äº§å“æ—¶ï¼Œè¯·åŸºäºä¸Šè¿°èº«ä»½é”šç‚¹è¿›è¡Œæè¿°ï¼Œç¡®ä¿æ‰€æœ‰é•œå¤´ä¸­çš„ä¸»ä½“ä¿æŒä¸€è‡´ã€‚` : ''}

è¯·æä¾›ä¸“ä¸šçš„å™äº‹ç»“æ„åˆ†æï¼Œè¾“å‡ºä¸¥æ ¼çš„JSONæ ¼å¼ï¼š

{
  "genre": "è§†é¢‘ç±»å‹ï¼ˆdrama/action/comedy/documentary/fantasy/sci-fiç­‰ï¼‰",
  "narrative_structure": "å™äº‹ç»“æ„ï¼ˆlinear/flashback/parallel/circularï¼‰",
  "characters": [
    {
      "id": "character_001",
      "name": "è§’è‰²åç§°æˆ–æè¿°",
      "physical_description": "è¯¦ç»†å¤–è²Œæè¿°ï¼šå¹´é¾„ã€æ€§åˆ«ã€ä½“å‹ã€è‚¤è‰²ã€å‘å‹ã€ç‰¹å¾ç­‰",
      "clothing_style": "æœè£…é£æ ¼å’Œé¢œè‰²æ­é…",
      "distinctive_features": ["ç‹¬ç‰¹ç‰¹å¾1", "ç‹¬ç‰¹ç‰¹å¾2", "è¡¨æƒ…ä¹ æƒ¯"],
      "consistency_keywords": ["å…³é”®æè¿°è¯1", "å…³é”®è¯2", "ç”¨äºä¿æŒä¸€è‡´æ€§"]
    }
  ],
  "setting": {
    "time_period": "æ—¶é—´è®¾å®šï¼ˆç°ä»£/å†å²/æœªæ¥ç­‰ï¼‰",
    "location": "åœ°ç‚¹è®¾å®šï¼ˆå®¤å†…/å®¤å¤–/å…·ä½“ç¯å¢ƒï¼‰",
    "atmosphere": "ç¯å¢ƒæ°›å›´æè¿°",
    "lighting_conditions": "ä¸»è¦å…‰ç…§æ¡ä»¶"
  },
  "emotional_arc": [
    {
      "timestamp": 0,
      "emotional_state": "å¼€å§‹æƒ…æ„ŸçŠ¶æ€",
      "intensity": 0.5,
      "visual_cues": "è§†è§‰è¡¨ç°è¦ç´ "
    }
  ],
  "key_themes": ["ä¸»è¦ä¸»é¢˜1", "ä¸»è¦ä¸»é¢˜2"],
  "visual_style": {
    "color_scheme": "ä¸»è‰²è°ƒæ–¹æ¡ˆï¼ˆwarm/cool/vibrant/mutedç­‰ï¼‰",
    "cinematography_style": "æ‘„å½±é£æ ¼ï¼ˆcinematic/documentary/artistic/commercialï¼‰",
    "movement_style": "é•œå¤´è¿åŠ¨åå¥½ï¼ˆstatic/dynamic/smooth/energeticï¼‰",
    "artistic_reference": "è‰ºæœ¯å‚è€ƒé£æ ¼"
  },
  "pacing_rhythm": {
    "overall_tempo": "æ•´ä½“èŠ‚å¥ï¼ˆslow/medium/fastï¼‰",
    "rhythm_changes": [
      {
        "at_second": 15,
        "new_tempo": "medium",
        "reason": "å™äº‹éœ€è¦æˆ–æƒ…æ„Ÿè½¬æŠ˜"
      }
    ]
  }
}

è¦æ±‚ï¼š
1. è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆJSONæ ¼å¼
2. æ‰€æœ‰å­—ç¬¦ä¸²ä½¿ç”¨åŒå¼•å·
3. è§’è‰²æè¿°è¦è¯¦ç»†å…·ä½“ï¼Œä¾¿äºè§†è§‰ç”Ÿæˆ
4. è€ƒè™‘è§†é¢‘è¿è´¯æ€§çš„éœ€æ±‚
5. å¦‚æœè¾“å…¥è¾ƒç®€å•ï¼Œè¯·åˆç†æ¨æµ‹å’Œæ‰©å±•å†…å®¹
    `;

    try {
      const response = await this.gemini.invoke(analysisPrompt);
      const content = response.content as string;

      // æ¸…ç†å’Œè§£æJSON
      const cleanedContent = content.replace(/```json\n?/g, "").replace(/```/g, "").trim();
      const analysis = JSON.parse(cleanedContent) as NarrativeAnalysis;

      // å°†å›¾ç‰‡åˆ†æç»“æœæ·»åŠ åˆ°åˆ†æç»“æœä¸­
      if (imageAnalysis) {
        analysis.identity_anchors = imageAnalysis.identityAnchors;
        analysis.subject_type = imageAnalysis.subjectType;
        analysis.subject_descriptor = imageAnalysis.subjectDescriptor;
      }

      console.log('âœ… å™äº‹åˆ†æå®Œæˆ:', {
        genre: analysis.genre,
        structure: analysis.narrative_structure,
        charactersCount: analysis.characters?.length || 0,
        emotionalPoints: analysis.emotional_arc?.length || 0,
        visualStyle: analysis.visual_style?.cinematography_style,
        hasIdentityAnchors: !!imageAnalysis,
        subjectType: analysis.subject_type,
        subjectDescriptor: analysis.subject_descriptor
      });

      return analysis;
    } catch (error) {
      console.error('âŒ Geminiå™äº‹åˆ†æå¤±è´¥:', error);
      // è¿”å›åŸºç¡€åˆ†æç»“æœä½œä¸ºé™çº§æ–¹æ¡ˆ
      return this.generateBasicAnalysis(userInput);
    }
  }

  /**
   * æå–å‚è€ƒå›¾ç‰‡çš„èº«ä»½é”šç‚¹ï¼ˆIdentity Anchorsï¼‰
   *
   * åŸºäº VEO 3.1 æœ€ä½³å®è·µ (2025):
   * - åªæå–ä¸»ä½“çš„æŒä¹…åŒ–ç‰¹å¾ï¼ˆé¢œè‰²ã€æè´¨ã€é£æ ¼ç­‰ï¼‰
   * - ä¸æè¿°åœºæ™¯ã€å…‰ç…§ã€æ„å›¾
   * - ä½¿ç”¨é€šç”¨æœ¯è¯­ï¼ˆ"the subject", "the product", "the character"ï¼‰
   * - ç”Ÿæˆå¯åœ¨æ‰€æœ‰é•œå¤´ä¸­é‡å¤ä½¿ç”¨çš„æè¿°
   */
  private async analyzeReferenceImages(imageUrls: string[]): Promise<ImageAnalysisResult | null> {
    try {
      // ä¸‹è½½ç¬¬ä¸€å¼ å›¾ç‰‡ï¼ˆä¸»è¦å‚è€ƒï¼‰
      const imageUrl = imageUrls[0];
      console.log(`ğŸ” æå–å‚è€ƒå›¾ç‰‡çš„èº«ä»½é”šç‚¹: ${imageUrl}`);

      const imageResponse = await fetch(imageUrl);
      if (!imageResponse.ok) {
        console.warn(`âš ï¸ æ— æ³•ä¸‹è½½å‚è€ƒå›¾ç‰‡: ${imageResponse.statusText}`);
        return null;
      }

      const imageBuffer = await imageResponse.arrayBuffer();
      const base64Image = Buffer.from(imageBuffer).toString('base64');

      // ä½¿ç”¨Gemini Visionæå–èº«ä»½é”šç‚¹
      const analysisPrompt = `Analyze this image and extract the subject's identity information for video generation.

**TASK 1: Identify Subject Type and Descriptor**
First, determine:
1. **Is it a PERSON or an OBJECT?**
2. **Provide a SHORT descriptor for Shot 1 prefix**:
   - If PERSON: Use "the man", "the woman", "the boy", "the girl", "the people", or "the person"
   - If OBJECT: Use the specific object name (e.g., "necklace", "sports car", "coffee cup", "handbag")

**TASK 2: Extract Identity Anchors**
Then provide detailed identity anchors following VEO 3.1 best practices:
- Focus ONLY on the main subject (person, product, or object)
- Extract persistent identity features that remain constant across video shots
- DO NOT describe scene, lighting, camera angle, or composition
- DO NOT describe actions or movements

**Key Physical Features to extract**:
   - For products: material, color, shape, design style, distinctive details
   - For people: gender, approximate age, hair (color, length, style), clothing (type, color)
   - For objects: color, material, size category, distinctive patterns

**Output format** (strict JSON):
{
  "subject_type": "person" or "object",
  "subject_descriptor": "the woman" or "necklace" (SHORT descriptor for Shot 1),
  "identity_anchors": "The subject is a [type] with [key feature 1], [key feature 2], [key feature 3]. Notable characteristics: [distinctive detail 1], [distinctive detail 2]."
}

**Example outputs**:
{
  "subject_type": "object",
  "subject_descriptor": "necklace",
  "identity_anchors": "The product is a diamond necklace with graduated sizing, silver platinum chain, and brilliant-cut stones. Notable characteristics: intricate metalwork, sparkling facets."
}

{
  "subject_type": "person",
  "subject_descriptor": "the woman",
  "identity_anchors": "The character is a woman in her late 20s with shoulder-length wavy black hair, wearing a green bomber jacket and gold hoop earrings. Notable characteristics: confident posture, natural makeup."
}

{
  "subject_type": "object",
  "subject_descriptor": "sports car",
  "identity_anchors": "The subject is a red sports car with sleek aerodynamic design, glossy paint finish, and distinctive front grille. Notable characteristics: low profile, aggressive styling."
}

Provide ONLY the JSON output, nothing else.`;

      const visionResult = await this.visionModel.generateContent([
        analysisPrompt,
        {
          inlineData: {
            data: base64Image,
            mimeType: imageResponse.headers.get('content-type') || 'image/jpeg'
          }
        }
      ]);

      const responseText = visionResult.response.text().trim();
      
      // æ¸…ç†JSONå“åº”
      const cleanedResponse = responseText.replace(/```json\n?/g, "").replace(/```/g, "").trim();
      const analysisResult = JSON.parse(cleanedResponse);
      
      console.log(`âœ… å›¾ç‰‡åˆ†æå®Œæˆ:`, {
        subjectType: analysisResult.subject_type,
        descriptor: analysisResult.subject_descriptor,
        anchors: analysisResult.identity_anchors.substring(0, 100) + '...'
      });

      return {
        identityAnchors: analysisResult.identity_anchors,
        subjectType: analysisResult.subject_type,
        subjectDescriptor: analysisResult.subject_descriptor
      };
    } catch (error) {
      console.error('âŒ èº«ä»½é”šç‚¹æå–å¤±è´¥:', error);
      return null;
    }
  }

  async generateEnhancedScenes(
    analysis: NarrativeAnalysis,
    targetDuration: number,
    ratio: string,
    hasUserUploadedImage: boolean = false
  ): Promise<EnhancedScenePlan[]> {
    console.log('ğŸ¬ ä½¿ç”¨Gemini 2.5 Flashç”Ÿæˆå¢å¼ºåœºæ™¯è§„åˆ’...');

    // è®¡ç®—åˆç†çš„åœºæ™¯æ•°é‡
    const segmentCount = this.calculateOptimalSegments(targetDuration);
    const segmentDuration = targetDuration / segmentCount;

    // å¦‚æœæœ‰èº«ä»½é”šç‚¹ï¼Œç”Ÿæˆç‰¹æ®ŠæŒ‡ä»¤
    const identityAnchorsNote = analysis.identity_anchors ? `
âš ï¸ **CRITICAL - IDENTITY ANCHORS (Character/Product Consistency)**:
${analysis.identity_anchors}

**MANDATORY REQUIREMENTS for ALL scenes**:
1. Include the identity anchor description in EVERY scene prompt
2. Use consistent descriptor: "${analysis.identity_anchors.split('.')[0]}." at the start of each scene
3. Maintain visual consistency: same colors, same materials, same distinctive features
4. Example: "${analysis.identity_anchors.split('.')[0]}. Camera slowly zooms in..."
5. DO NOT alter or reinterpret the subject's appearance across scenes

This ensures VEO 3.1 generates consistent subject appearance across all video segments.
` : '';

    const scenePrompt = `
åŸºäºä»¥ä¸‹æ·±åº¦å™äº‹åˆ†æï¼Œç”Ÿæˆ${segmentCount}ä¸ªé«˜è´¨é‡çš„è§†é¢‘åœºæ™¯åˆ†é•œè®¡åˆ’ï¼š

å™äº‹åˆ†æï¼š
${JSON.stringify(analysis, null, 2)}

æŠ€æœ¯å‚æ•°ï¼š
- ç›®æ ‡æ€»æ—¶é•¿ï¼š${targetDuration}ç§’
- åœºæ™¯æ•°é‡ï¼š${segmentCount}ä¸ª
- æ¯æ®µå¹³å‡æ—¶é•¿ï¼š${segmentDuration.toFixed(1)}ç§’
- è§†é¢‘æ¯”ä¾‹ï¼š${ratio}
${identityAnchorsNote}

è¯·ç”Ÿæˆè¯¦ç»†çš„åœºæ™¯åˆ†é•œè®¡åˆ’ï¼Œè¾“å‡ºJSONæ ¼å¼ï¼š

{
  "scenes": [
    {
      "id": "scene_001",
      "sequence_number": 1,
      "duration_seconds": ${segmentDuration.toFixed(1)},
      "narrative_purpose": "exposition",
      "visual_description": "å®Œæ•´çš„åœºæ™¯è§†è§‰æè¿°ï¼ŒåŒ…å«ç¯å¢ƒã€è§’è‰²ã€åŠ¨ä½œã€æƒ…ç»ª",
      "camera_movement": {
        "type": "static|pan_left|pan_right|tilt_up|tilt_down|zoom_in|zoom_out|dolly_forward|dolly_back|orbit|crane_up|crane_down",
        "speed": "slow|medium|fast",
        "description": "å…·ä½“çš„é•œå¤´è¿åŠ¨æè¿°"
      },
      "lighting_style": {
        "type": "natural|cinematic|dramatic|soft|hard|golden_hour|blue_hour",
        "direction": "front|back|side|top|bottom|rim|mixed",
        "mood": "bright|dim|moody|ethereal|harsh|warm"
      },
      "color_palette": {
        "primary_colors": ["#FF6B6B", "#4ECDC4"],
        "secondary_colors": ["#45B7D1"],
        "mood_descriptor": "warm|cool|neutral|vibrant|muted|desaturated"
      },
      "characters_present": ["character_001"],
      "consistency_anchors": [
        {
          "type": "character",
          "reference_description": "ç¡®ä¿è§’è‰²å¤–è§‚ä¸€è‡´æ€§çš„å…³é”®æè¿°",
          "importance_weight": 0.9,
          "consistency_prompt": "ç”¨äºä¿æŒè§†è§‰ä¸€è‡´æ€§çš„å…³é”®æç¤ºè¯"
        }
      ],
      "runway_prompt": "ä¸ºRunwayä¼˜åŒ–çš„è‹±æ–‡ç”Ÿæˆæç¤ºè¯ï¼ŒåŒ…å«æ‰€æœ‰é‡è¦è§†è§‰ä¿¡æ¯ï¼Œç¡®ä¿é«˜è´¨é‡ç”Ÿæˆ"
    }
  ]
}

é‡è¦è¦æ±‚ï¼š
1. æ¯ä¸ªsceneçš„runway_promptå¿…é¡»æ˜¯é«˜è´¨é‡çš„è‹±æ–‡æè¿°
2. ç¡®ä¿è§’è‰²æè¿°åœ¨å„åœºæ™¯é—´ä¿æŒä¸€è‡´
3. è€ƒè™‘åœºæ™¯é—´çš„è‡ªç„¶è¿‡æ¸¡å’Œæƒ…æ„Ÿè¿è´¯
4. æ ¹æ®å™äº‹å¼§çº¿å®‰æ’narrative_purpose
5. è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆJSONæ ¼å¼
6. runway_promptè¦è¯¦ç»†ä½†ç®€æ´ï¼Œé€‚åˆè§†é¢‘ç”ŸæˆAIç†è§£
    `;

    try {
      const response = await this.gemini.invoke(scenePrompt);
      const content = response.content as string;

      const cleanedContent = content.replace(/```json\n?/g, "").replace(/```/g, "").trim();
      const sceneResult = JSON.parse(cleanedContent);

      console.log('âœ… åœºæ™¯è§„åˆ’å®Œæˆ:', {
        scenesGenerated: sceneResult.scenes?.length || 0,
        totalDuration: sceneResult.scenes?.reduce((sum: number, s: any) => sum + s.duration_seconds, 0) || 0
      });

      return sceneResult.scenes as EnhancedScenePlan[];
    } catch (error) {
      console.error('âŒ Geminiåœºæ™¯è§„åˆ’å¤±è´¥:', error);
      return this.generateBasicScenes(analysis, targetDuration, segmentCount);
    }
  }

  async optimizeForConsistency(
    scenes: EnhancedScenePlan[],
    analysis: NarrativeAnalysis
  ): Promise<EnhancedScenePlan[]> {
    console.log('ğŸ”„ ä½¿ç”¨Geminiä¼˜åŒ–è§†è§‰ä¸€è‡´æ€§...');

    const optimizationPrompt = `
ä½œä¸ºä¸“ä¸šçš„è§†é¢‘ä¸€è‡´æ€§ä¼˜åŒ–ä¸“å®¶ï¼Œè¯·ä¼˜åŒ–ä»¥ä¸‹åœºæ™¯åºåˆ—çš„è§†è§‰ä¸€è‡´æ€§ï¼š

åŸå§‹å™äº‹åˆ†æï¼š
${JSON.stringify(analysis, null, 2)}

å½“å‰åœºæ™¯è®¡åˆ’ï¼š
${JSON.stringify(scenes, null, 2)}

è¯·ä¼˜åŒ–æ¯ä¸ªåœºæ™¯çš„runway_promptï¼Œç¡®ä¿ï¼š
1. è§’è‰²å¤–è§‚åœ¨æ‰€æœ‰åœºæ™¯ä¸­å®Œå…¨ä¸€è‡´ï¼ˆæœè£…ã€å‘å‹ã€å¹´é¾„ã€ä½“å‹ç­‰ï¼‰
2. ç¯å¢ƒé£æ ¼å’Œè‰²è°ƒä¿æŒè¿è´¯
3. å…‰ç…§é£æ ¼åè°ƒç»Ÿä¸€
4. é•œå¤´é£æ ¼ä¿æŒä¸€è‡´æ€§

è¯·è¾“å‡ºä¼˜åŒ–åçš„JSONæ ¼å¼ï¼š
{
  "optimized_scenes": [
    {
      "id": "scene_001",
      "optimized_runway_prompt": "ä¼˜åŒ–åçš„è‹±æ–‡æç¤ºè¯ï¼Œå¼ºè°ƒä¸€è‡´æ€§è¦ç´ "
    }
  ]
}

è¦æ±‚ï¼š
1. ä¿æŒåŸæœ‰çš„åˆ›æ„å’Œå™äº‹é€»è¾‘
2. å¼ºåŒ–è§†è§‰ä¸€è‡´æ€§å…ƒç´ 
3. ç¡®ä¿è‹±æ–‡æç¤ºè¯è´¨é‡
4. è¾“å‡ºæœ‰æ•ˆJSONæ ¼å¼
    `;

    try {
      const response = await this.gemini.invoke(optimizationPrompt);
      const content = response.content as string;

      const cleanedContent = content.replace(/```json\n?/g, "").replace(/```/g, "").trim();
      const optimizationResult = JSON.parse(cleanedContent);

      // åº”ç”¨ä¼˜åŒ–ç»“æœ
      const optimizedScenes = scenes.map(scene => {
        const optimized = optimizationResult.optimized_scenes.find((opt: any) => opt.id === scene.id);
        if (optimized) {
          return {
            ...scene,
            runway_prompt: optimized.optimized_runway_prompt
          };
        }
        return scene;
      });

      console.log('âœ… ä¸€è‡´æ€§ä¼˜åŒ–å®Œæˆ');
      return optimizedScenes;
    } catch (error) {
      console.error('âŒ ä¸€è‡´æ€§ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹åœºæ™¯:', error);
      return scenes;
    }
  }

  private calculateOptimalSegments(targetDuration: number): number {
    // æ ¹æ®LLMScheAgent.mdçš„FramePackç­–ç•¥ä¼˜åŒ–åˆ†æ®µ
    if (targetDuration <= 20) {
      return 3; // çŸ­è§†é¢‘3æ®µ
    } else if (targetDuration <= 40) {
      return Math.ceil(targetDuration / 8); // æ¯æ®µçº¦8ç§’
    } else {
      return Math.min(8, Math.ceil(targetDuration / 7)); // é•¿è§†é¢‘æœ€å¤š8æ®µï¼Œæ¯æ®µçº¦7ç§’
    }
  }

  private generateBasicAnalysis(userInput: string): NarrativeAnalysis {
    // é™çº§æ–¹æ¡ˆï¼šåŸºç¡€åˆ†æç»“æ„
    return {
      genre: "general",
      narrative_structure: "linear",
      characters: [{
        id: "character_001",
        name: "main subject",
        physical_description: "person in the scene",
        clothing_style: "appropriate attire",
        distinctive_features: ["expressive face"],
        consistency_keywords: ["consistent appearance"]
      }],
      setting: {
        time_period: "modern",
        location: "appropriate setting",
        atmosphere: "natural",
        lighting_conditions: "natural lighting"
      },
      emotional_arc: [{
        timestamp: 0,
        emotional_state: "neutral",
        intensity: 0.5,
        visual_cues: "natural expression"
      }],
      key_themes: ["main narrative"],
      visual_style: {
        color_scheme: "natural",
        cinematography_style: "cinematic",
        movement_style: "smooth",
        artistic_reference: "professional"
      },
      pacing_rhythm: {
        overall_tempo: "medium",
        rhythm_changes: []
      }
    };
  }

  private generateBasicScenes(
    analysis: NarrativeAnalysis,
    targetDuration: number,
    segmentCount: number
  ): EnhancedScenePlan[] {
    // é™çº§æ–¹æ¡ˆï¼šåŸºç¡€åœºæ™¯ç”Ÿæˆ
    const segmentDuration = targetDuration / segmentCount;
    const scenes: EnhancedScenePlan[] = [];

    for (let i = 0; i < segmentCount; i++) {
      scenes.push({
        id: `scene_${(i + 1).toString().padStart(3, '0')}`,
        sequence_number: i + 1,
        duration_seconds: segmentDuration,
        narrative_purpose: i === 0 ? 'exposition' : i === segmentCount - 1 ? 'resolution' : 'rising_action',
        visual_description: `Scene ${i + 1} of the narrative`,
        camera_movement: {
          type: i % 2 === 0 ? "static" : "dolly_forward",
          speed: "medium",
          description: "smooth camera movement"
        },
        lighting_style: {
          type: "cinematic",
          direction: "front",
          mood: "natural"
        },
        color_palette: {
          primary_colors: ["#8B9DC3", "#F7DBA7"],
          secondary_colors: ["#F69E7B"],
          mood_descriptor: "warm"
        },
        characters_present: ["character_001"],
        consistency_anchors: [{
          type: "character",
          reference_description: "consistent character appearance",
          importance_weight: 0.8,
          consistency_prompt: "maintain visual consistency"
        }],
        runway_prompt: `Professional cinematic scene, high quality, detailed environment, consistent lighting, segment ${i + 1} of narrative sequence`
      });
    }

    return scenes;
  }
}

// ==================== ä¸»å¯¼å‡ºå‡½æ•° ====================
export async function generateEnhancedShotPlan(
  userPrompt: string,
  targetSeconds: number = 30,
  ratio: string = '1280:768',
  referenceImages?: string[] // æ–°å¢ï¼šç”¨æˆ·ä¸Šä¼ çš„å‚è€ƒå›¾ç‰‡
): Promise<ShotPlan> {
  console.log(`ğŸš€ å¯åŠ¨Gemini 2.5 Flashå¢å¼ºå™äº‹åˆ†è§£å™¨`);
  console.log(`ğŸ“ å¤„ç†è¾“å…¥: "${userPrompt}" (${targetSeconds}s, ${ratio})`);

  if (referenceImages && referenceImages.length > 0) {
    console.log(`ğŸ–¼ï¸ æ£€æµ‹åˆ° ${referenceImages.length} å¼ å‚è€ƒå›¾ç‰‡ï¼Œå°†åœ¨é•œå¤´è§„åˆ’ä¸­ä½¿ç”¨`);
  }

  const planner = new GeminiEnhancedShotPlanner();

  try {
    // æ­¥éª¤1ï¼šæ·±åº¦å™äº‹åˆ†æï¼ˆåŒ…å«å‚è€ƒå›¾ç‰‡åˆ†æï¼‰
    console.log('ğŸ§  ç¬¬ä¸€æ­¥ï¼šæ·±åº¦å™äº‹åˆ†æ');
    const narrativeAnalysis = await planner.analyzeUserInput(userPrompt, referenceImages);

    // æ­¥éª¤2ï¼šç”Ÿæˆå¢å¼ºåœºæ™¯è®¡åˆ’
    console.log('ğŸ¬ ç¬¬äºŒæ­¥ï¼šç”Ÿæˆå¢å¼ºåœºæ™¯è®¡åˆ’');
    const hasUserUploadedImage = (referenceImages && referenceImages.length > 0);
    const scenePlans = await planner.generateEnhancedScenes(
      narrativeAnalysis,
      targetSeconds,
      ratio,
      hasUserUploadedImage
    );

    // æ­¥éª¤3ï¼šä¼˜åŒ–è§†è§‰ä¸€è‡´æ€§
    console.log('ğŸ”„ ç¬¬ä¸‰æ­¥ï¼šä¼˜åŒ–è§†è§‰ä¸€è‡´æ€§');
    const optimizedScenes = await planner.optimizeForConsistency(scenePlans, narrativeAnalysis);

    // æ­¥éª¤4ï¼šè½¬æ¢ä¸ºæ ‡å‡†è¾“å‡ºæ ¼å¼
    console.log('ğŸ“‹ ç¬¬å››æ­¥ï¼šæ ¼å¼åŒ–è¾“å‡º');
    const shots = optimizedScenes.map((scene, index) => {
      let prompt = scene.runway_prompt;
      
      // å¦‚æœæ˜¯ shot 1 ä¸”æœ‰å‚è€ƒå›¾ç‰‡ï¼Œåœ¨æç¤ºè¯å‰é¢å¼ºè¡ŒåŠ å…¥ä¸»ä½“æè¿° + "based on the picture uploaded as reference"
      if (index === 0 && narrativeAnalysis.subject_descriptor) {
        const prefix = `${narrativeAnalysis.subject_descriptor} based on the picture uploaded as reference, `;
        prompt = prefix + prompt;
        console.log(`âœ¨ Shot 1 æ·»åŠ å‚è€ƒå›¾ç‰‡å‰ç¼€: "${prefix}"`);
      }
      
      return {
        id: index + 1,
        prompt: prompt,
        duration_s: Math.round(scene.duration_seconds),
        camera: scene.camera_movement.description || scene.camera_movement.type
      };
    });

    const totalDuration = shots.reduce((sum, shot) => sum + shot.duration_s, 0);

    const result: ShotPlan = {
      ratio,
      total_seconds: totalDuration,
      shots
    };

    console.log('ğŸ‰ Gemini 2.5 Flashå¢å¼ºæ–¹æ¡ˆå®Œæˆ:', {
      totalShots: shots.length,
      totalDuration: totalDuration,
      averageQuality: "enhanced",
      hasImageToVideo: hasUserUploadedImage,
      shots: shots.map(s => ({ id: s.id, duration: s.duration_s, camera: s.camera }))
    });

    return result;

  } catch (error) {
    console.error('âŒ Gemini 2.5 Flashå¢å¼ºæ–¹æ¡ˆå¤±è´¥:', error);
    throw new Error(`Enhanced shot planning failed: ${error.message}`);
  }
}

// å¯¼å‡ºç±»å‹å®šä¹‰
export type {
  ShotPlan,
  NarrativeAnalysis,
  EnhancedScenePlan,
  CharacterProfile
};