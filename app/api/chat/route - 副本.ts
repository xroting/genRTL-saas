import { NextRequest, NextResponse } from "next/server";
import { createSupabaseServer } from "@/lib/supabase/server";
import OpenAI from "openai";
import { ProxyAgent } from "undici";

const systemPrompt = `ä½ æ˜¯ä¸€åä¼˜ç§€çš„FPGAå’ŒASICæ•°å­—å‰ç«¯é«˜çº§å·¥ç¨‹å¸ˆï¼Œç²¾é€šVerilog/SystemVerilog HDLè¯­è¨€è®¾è®¡ã€‚

## ğŸš¨ é‡è¦ï¼šä»£ç è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

**æ— è®ºæ˜¯åˆ›å»ºæ–°æ–‡ä»¶è¿˜æ˜¯ä¿®æ”¹ç°æœ‰æ–‡ä»¶ï¼Œéƒ½å¿…é¡»ç›´æ¥è¾“å‡ºå®Œæ•´çš„ä»£ç ï¼Œä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š**

\`\`\`verilog
src/filename.v
[å®Œæ•´çš„æ¨¡å—ä»£ç ]
\`\`\`

**ğŸš¨ è·¯å¾„æ ¼å¼è¦æ±‚ï¼ˆæå…¶é‡è¦ï¼‰ï¼š**
- âœ… å¿…é¡»ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼šsrc/uart.v, src/top.v, lib/utils.v
- âŒ ç¦æ­¢ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼še:\\uart\\src\\uart.v, /home/user/src/uart.v
- âŒ ç¦æ­¢ä½¿ç”¨ç›˜ç¬¦è·¯å¾„ï¼šd:/project/src/uart.v

**ç¦æ­¢ä½¿ç”¨ä»»ä½•å·¥å…·ï¼** ä¸è¦ä½¿ç”¨read_fileã€write_fileç­‰å·¥å…·ï¼Œå³ä½¿ç”¨æˆ·è¦æ±‚ä¿®æ”¹ç°æœ‰æ–‡ä»¶ï¼Œä¹Ÿåº”è¯¥ç›´æ¥è¾“å‡ºå®Œæ•´çš„æ–°ä»£ç ã€‚

## å·¥ä½œæµç¨‹

1. ç†è§£ç”¨æˆ·éœ€æ±‚
2. è®¾è®¡æˆ–ä¿®æ”¹æ¨¡å—  
3. **ç›´æ¥è¾“å‡ºå®Œæ•´ä»£ç ï¼Œä½¿ç”¨Markdownä»£ç å—æ ¼å¼**
4. æ ¼å¼ï¼šä¸‰ä¸ªåå¼•å· + è¯­è¨€å + æ¢è¡Œ + ç›¸å¯¹è·¯å¾„ + æ¢è¡Œ + å®Œæ•´ä»£ç  + ä¸‰ä¸ªåå¼•å·

## ç¤ºä¾‹æ ¼å¼

\`\`\`verilog
src/counter.v
module counter(
  input wire clk,
  output reg [7:0] count
);
  always @(posedge clk) count <= count + 1;
endmodule
\`\`\`

## å…³é”®è¦æ±‚

- ä»£ç å¿…é¡»å®Œæ•´ï¼Œå¯ç›´æ¥ä½¿ç”¨
- æ¯ä¸ªä»£ç å—ç¬¬ä¸€è¡Œå¿…é¡»æ˜¯**ç›¸å¯¹è·¯å¾„**ï¼ˆsrc/xxx.vï¼‰
- ä¸è¦ä½¿ç”¨ä»»ä½•å·¥å…·
- ç›´æ¥è¾“å‡ºå®Œæ•´ä»£ç `;

// Chat API for genRTL AI Assistant
// This endpoint handles chat conversations between the frontend and AI backend

// CORS headers for VS Code webview
const corsHeaders = {
  "Access-Control-Allow-Origin": "*",
  "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
  "Access-Control-Allow-Headers": "Content-Type, Authorization",
};

interface ChatMessage {
  role: "user" | "assistant" | "system";
  content: string;
}

interface ChatRequest {
  messages: ChatMessage[];
  model?: string;
  stream?: boolean;
  temperature?: number;
  max_tokens?: number;
  tools?: any[]; // OpenAI tool definitions
}

// Handle OPTIONS request for CORS preflight
export async function OPTIONS(req: NextRequest) {
  return NextResponse.json({}, { headers: corsHeaders });
}

export async function POST(req: NextRequest) {
  try {
    const requestBody: ChatRequest = await req.json();
    console.log("ğŸ“¥ Received chat request:", {
      messageCount: requestBody.messages?.length || 0,
      model: requestBody.model,
      stream: requestBody.stream,
      toolsCount: requestBody.tools?.length || 0, // Log tools count
    });

    const { messages, model = "gpt-4o-mini", stream = false, temperature = 0.7, max_tokens = 32768, tools } = requestBody;

    if (!messages || messages.length === 0) {
      return NextResponse.json(
        { error: "messages are required" },
        { status: 400, headers: corsHeaders }
      );
    }

    // Try to get user from Authorization header (for mobile/extension) or cookie (for web)
    const authHeader = req.headers.get("authorization");
    let user = null;
    let supa;

    if (authHeader?.startsWith("Bearer ")) {
      // Extension/Mobile: use Bearer token
      const token = authHeader.substring(7);
      const { createClient } = await import("@supabase/supabase-js");

      const authClient = createClient(
        process.env.NEXT_PUBLIC_SUPABASE_URL!,
        process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
      );
      const { data: { user: tokenUser } } = await authClient.auth.getUser(token);
      user = tokenUser;

      supa = createClient(
        process.env.NEXT_PUBLIC_SUPABASE_URL!,
        process.env.SUPABASE_SERVICE_ROLE_KEY!,
        {
          auth: {
            autoRefreshToken: false,
            persistSession: false,
          },
        }
      );
    } else {
      // Web: use cookie
      supa = await createSupabaseServer();
      const { data: { user: cookieUser } } = await supa.auth.getUser();
      user = cookieUser;
    }

    // For now, allow unauthenticated users for development
    // In production, you should require authentication
    if (!user) {
      console.log("âš ï¸ Unauthenticated chat request - allowing for development");
    }

    // Call OpenAI API using official SDK with proxy support
    const openaiApiKey = process.env.OPENAI_API_KEY;
    if (!openaiApiKey) {
      return NextResponse.json(
        { error: "OpenAI API key not configured" },
        { status: 500, headers: corsHeaders }
      );
    }

    // Create OpenAI client with proxy configuration
    const proxyUrl = process.env.HTTPS_PROXY || process.env.HTTP_PROXY;
    const openaiConfig: any = {
      apiKey: openaiApiKey,
    };

    // Add undici ProxyAgent if proxy is configured (compatible with Node.js fetch)
    if (proxyUrl) {
      console.log("ğŸŒ Using proxy:", proxyUrl);
      const proxyAgent = new ProxyAgent(proxyUrl);
      // Use the dispatcher option for undici-based fetch
      openaiConfig.httpAgent = proxyAgent;
      // @ts-ignore - OpenAI SDK supports custom fetch options
      openaiConfig.fetch = (url: any, init: any) => {
        return fetch(url, {
          ...init,
          // @ts-ignore
          dispatcher: proxyAgent,
        });
      };
    }

    const openai = new OpenAI(openaiConfig);

    console.log("ğŸ¤– Calling OpenAI API via SDK with undici ProxyAgent...");

    // âœ… Add system prompt and few-shot examples
    // ğŸš¨ CRITICAL: Few-shot examples must use RELATIVE paths (src/file.v) NOT absolute paths
    const messagesWithSystem: ChatMessage[] = [
      { role: "system", content: systemPrompt },
      // Few-shot example 1: Create new UART file - ä½¿ç”¨ç›¸å¯¹è·¯å¾„ src/uart.v
      { role: "user", content: "è¯·å¸®æˆ‘åˆ›å»ºä¸€ä¸ªUARTæ¨¡å—" },
      { role: "assistant", content: "å¥½çš„ï¼Œæˆ‘å°†ä¸ºæ‚¨åˆ›å»ºä¸€ä¸ªUARTæ¨¡å—ã€‚ä»¥ä¸‹æ˜¯ä»£ç ï¼š\n\n```verilog\nsrc/uart.v\nmodule uart(\n  input wire clk,\n  input wire reset,\n  output reg tx\n);\n\nparameter BAUD_RATE = 115200;\nparameter CLK_FREQ = 100000000;\n\nlocalparam DIVISOR = CLK_FREQ / BAUD_RATE;\nreg [15:0] counter;\n\nalways @(posedge clk) begin\n  if (reset) counter <= 0;\n  else counter <= counter + 1;\nend\n\nendmodule\n```\n\nè¿™ä¸ªUARTæ¨¡å—å·²ç»åŒ…å«äº†åŸºæœ¬çš„æ³¢ç‰¹ç‡åˆ†é¢‘å™¨ã€‚" },
      // Few-shot example 2: Modify existing file - è¾“å‡ºå®Œæ•´ä»£ç ï¼Œä½¿ç”¨ç›¸å¯¹è·¯å¾„
      { role: "user", content: "è¯·å°†æ—¶é’Ÿé¢‘ç‡æ”¹ä¸º50MHz" },
      { role: "assistant", content: "å¥½çš„ï¼Œæˆ‘å°†æ›´æ–°UARTæ¨¡å—çš„æ—¶é’Ÿé¢‘ç‡ä¸º50MHzã€‚ä»¥ä¸‹æ˜¯ä¿®æ”¹åçš„å®Œæ•´ä»£ç ï¼š\n\n```verilog\nsrc/uart.v\nmodule uart(\n  input wire clk,\n  input wire reset,\n  output reg tx\n);\n\nparameter BAUD_RATE = 115200;\nparameter CLK_FREQ = 50000000;  // å·²æ›´æ–°ä¸º50MHz\n\nlocalparam DIVISOR = CLK_FREQ / BAUD_RATE;\nreg [15:0] counter;\n\nalways @(posedge clk) begin\n  if (reset) counter <= 0;\n  else counter <= counter + 1;\nend\n\nendmodule\n```\n\næ—¶é’Ÿé¢‘ç‡å·²æ›´æ–°ä¸º50MHzã€‚" },
      ...messages,
    ];

    console.log("âœ… System prompt added, total messages:", messagesWithSystem.length);

    // ğŸ”¥ Use max_tokens from request, with a reasonable upper limit
    // For gpt-4.1-mini-2025-04-14, max output is 32768 tokens (verified by OpenAI API)
    const safeMaxTokens = Math.min(max_tokens, 32768);
    console.log(`ğŸ“Š Token limit: requested=${max_tokens}, using=${safeMaxTokens}`);

    // ğŸ”„ Retry configuration for OpenAI API calls
    const MAX_RETRIES = 3;
    const RETRY_DELAY_MS = 2000; // Start with 2 seconds

    if (stream) {
      // Streaming response using OpenAI SDK
      let lastError: any = null;
      
      for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
        try {
          if (attempt > 0) {
            const delay = RETRY_DELAY_MS * Math.pow(2, attempt - 1); // Exponential backoff
            console.log(`â³ Retry attempt ${attempt + 1}/${MAX_RETRIES} after ${delay}ms...`);
            await new Promise(resolve => setTimeout(resolve, delay));
          }

          const streamResponse = await openai.chat.completions.create({
            model,
            messages: messagesWithSystem,  // â† Use messages with system prompt
            temperature,
            max_tokens: safeMaxTokens, // Use safe token limit
            stream: true,
            // DO NOT pass tools - we want LLM to output code directly, not use tools
          });

          console.log("âœ… OpenAI stream started");

          // Convert OpenAI stream to SSE format
          const encoder = new TextEncoder();
          const readable = new ReadableStream({
            async start(controller) {
              try {
                let fullResponse = ''; // Track full response for debugging
                for await (const chunk of streamResponse) {
                  // Log first few chunks to see what's being returned
                  if (chunk.choices[0]?.delta?.content) {
                    fullResponse += chunk.choices[0].delta.content;
                    if (fullResponse.length < 500) {
                      console.log('ğŸ“ Response preview:', fullResponse);
                    }
                  }
                  const data = JSON.stringify(chunk);
                  const text = `data: ${data}\n\n`;
                  controller.enqueue(encoder.encode(text));
                }
                console.log('ğŸ“„ Full response length:', fullResponse.length, 'chars');
                console.log('ğŸ“„ Has code block:', fullResponse.includes('```'));
                controller.enqueue(encoder.encode('data: [DONE]\n\n'));
                controller.close();
              } catch (error) {
                console.error("âŒ Stream error:", error);
                controller.error(error);
              }
            },
          });

          return new Response(readable, {
            headers: {
              "Content-Type": "text/event-stream",
              "Cache-Control": "no-cache",
              "Connection": "keep-alive",
              ...corsHeaders,
            },
          });
        } catch (error: any) {
          lastError = error;
          console.error(`âŒ OpenAI API error (attempt ${attempt + 1}/${MAX_RETRIES}):`, error);
          
          // Don't retry on rate limit errors (429) - let the client handle backoff
          if (error.status === 429) {
            console.log("ğŸš« Rate limit error - not retrying");
            break;
          }
          
          // Don't retry on authentication errors (401, 403)
          if (error.status === 401 || error.status === 403) {
            console.log("ğŸš« Authentication error - not retrying");
            break;
          }
          
          // Retry on connection errors and 5xx server errors
          const shouldRetry = (
            error.code === 'ECONNRESET' ||
            error.code === 'ETIMEDOUT' ||
            error.code === 'ENOTFOUND' ||
            (error.status && error.status >= 500)
          );
          
          if (!shouldRetry || attempt === MAX_RETRIES - 1) {
            break;
          }
        }
      }
      
      // All retries failed
      const errorResponse = {
        error: "AI provider error",
        details: lastError?.message || "Unknown error",
        code: lastError?.code || "unknown",
        status: lastError?.status || 500,
      };
      
      return NextResponse.json(
        errorResponse,
        { status: lastError?.status || 500, headers: corsHeaders }
      );
    } else {
      // Non-streaming response using OpenAI SDK
      let lastError: any = null;
      
      for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
        try {
          if (attempt > 0) {
            const delay = RETRY_DELAY_MS * Math.pow(2, attempt - 1); // Exponential backoff
            console.log(`â³ Retry attempt ${attempt + 1}/${MAX_RETRIES} after ${delay}ms...`);
            await new Promise(resolve => setTimeout(resolve, delay));
          }

          const completion = await openai.chat.completions.create({
            model,
            messages: messagesWithSystem,  // â† Use messages with system prompt
            temperature,
            max_tokens: safeMaxTokens, // Use safe token limit
            // DO NOT pass tools - we want LLM to output code directly, not use tools
          });

          console.log("âœ… OpenAI API response received");

          return NextResponse.json({
            id: completion.id,
            choices: completion.choices,
            usage: completion.usage,
            model: completion.model,
          }, { headers: corsHeaders });
        } catch (error: any) {
          lastError = error;
          console.error(`âŒ OpenAI API error (attempt ${attempt + 1}/${MAX_RETRIES}):`, error);
          
          // Don't retry on rate limit errors (429) - let the client handle backoff
          if (error.status === 429) {
            console.log("ğŸš« Rate limit error - not retrying");
            break;
          }
          
          // Don't retry on authentication errors (401, 403)
          if (error.status === 401 || error.status === 403) {
            console.log("ğŸš« Authentication error - not retrying");
            break;
          }
          
          // Retry on connection errors and 5xx server errors
          const shouldRetry = (
            error.code === 'ECONNRESET' ||
            error.code === 'ETIMEDOUT' ||
            error.code === 'ENOTFOUND' ||
            (error.status && error.status >= 500)
          );
          
          if (!shouldRetry || attempt === MAX_RETRIES - 1) {
            break;
          }
        }
      }
      
      // All retries failed
      const errorResponse = {
        error: "AI provider error",
        details: lastError?.message || "Unknown error",
        code: lastError?.code || "unknown",
        status: lastError?.status || 500,
      };
      
      return NextResponse.json(
        errorResponse,
        { status: lastError?.status || 500, headers: corsHeaders }
      );
    }
  } catch (error) {
    console.error("âŒ Error in chat API:", error);
    return NextResponse.json(
      { error: "internal server error", details: String(error) },
      { status: 500, headers: corsHeaders }
    );
  }
}

// GET method to retrieve chat history (optional)
export async function GET(req: NextRequest) {
  try {
    const conversationId = req.nextUrl.searchParams.get("conversation_id");

    // For now, return empty history
    // In the future, you can implement conversation storage in Supabase
    return NextResponse.json({
      messages: [],
      conversation_id: conversationId,
    }, { headers: corsHeaders });
  } catch (error) {
    console.error("âŒ Error fetching chat history:", error);
    return NextResponse.json(
      { error: "internal server error" },
      { status: 500, headers: corsHeaders }
    );
  }
}
