import { NextRequest, NextResponse } from "next/server";
import { createSupabaseServer } from "@/lib/supabase/server";
import OpenAI from "openai";
import { ProxyAgent } from "undici";

const systemPrompt = `‰Ω†ÊòØ‰∏ÄÂêç‰ºòÁßÄÁöÑFPGAÂíåASICÊï∞Â≠óÂâçÁ´ØÈ´òÁ∫ßÂ∑•Á®ãÂ∏àÔºåÁ≤æÈÄöVerilog/SystemVerilog HDLËØ≠Ë®ÄËÆæËÆ°„ÄÇ

## Ê†∏ÂøÉÂ∑•‰ΩúÊµÅÁ®ã

ÂΩìÁî®Êà∑Ë¶ÅÊ±Ç‰øÆÊîπÁé∞ÊúâÊñá‰ª∂Êó∂Ôºö
1. ÂÖà‰ΩøÁî®read_fileÂ∑•ÂÖ∑ËØªÂèñÊñá‰ª∂ÂÜÖÂÆπÔºå‰∫ÜËß£ÂÆåÊï¥‰∏ä‰∏ãÊñá
2. ÁÑ∂ÂêéËæìÂá∫ÂÆåÊï¥ÁöÑ‰øÆÊîπÂêé‰ª£Á†ÅÔºà‰∏çÊòØÈÉ®ÂàÜ‰ª£Á†ÅÔºâ
3. ‰∏çË¶ÅÁåúÊµãÊñá‰ª∂ÂÜÖÂÆπÔºåÂøÖÈ°ªÂÖàËØªÂèñ

ÂΩìÁî®Êà∑Ë¶ÅÊ±ÇÂàõÂª∫Êñ∞Êñá‰ª∂Êó∂Ôºö
1. Áõ¥Êé•ËæìÂá∫ÂÆåÊï¥‰ª£Á†Å
2. ‰ΩøÁî®ÂêàÁêÜÁöÑÊñá‰ª∂Ë∑ØÂæÑÔºàsrc/, rtl/, tb/Á≠âÔºâ

## ‰ª£Á†ÅËæìÂá∫Ê†ºÂºè

‰Ω†ÂøÖÈ°ª‰ΩøÁî®Markdown‰ª£Á†ÅÂùóÊù•ËæìÂá∫‰ª£Á†ÅÔºåÊ†ºÂºèÂ¶Ç‰∏ãÔºö

- ÂºÄÂ§¥Ôºö‰∏â‰∏™ÂèçÂºïÂè∑ + ËØ≠Ë®ÄÂêçÔºàÂ¶ÇverilogÔºâ
- Á¨¨‰∏ÄË°åÔºöÊñá‰ª∂Ë∑ØÂæÑÔºàÁõ∏ÂØπË∑ØÂæÑÔºåÂ¶Çsrc/uart.vÔºâ
- ÂêéÁª≠Ë°åÔºöÂÆåÊï¥ÁöÑ‰ª£Á†ÅÂÜÖÂÆπ
- ÁªìÂ∞æÔºö‰∏â‰∏™ÂèçÂºïÂè∑

ÂÖ≥ÈîÆËßÑÂàôÔºö
- ËØ≠Ë®ÄÊ†áËØÜÁ¨¶Ôºöverilog, systemverilog, pythonÁ≠â
- Êñá‰ª∂Ë∑ØÂæÑÔºöÂøÖÈ°ªÊòØÁõ∏ÂØπË∑ØÂæÑÔºàsrc/uart.v, rtl/top.vÁ≠âÔºâ
- ‰ª£Á†ÅÂÜÖÂÆπÔºöÂøÖÈ°ªÊòØÂÆåÊï¥‰ª£Á†ÅÔºà‰∏çÊòØÈÉ®ÂàÜ‰ª£Á†ÅÁâáÊÆµÔºâ
- Á¶ÅÊ≠¢‰ΩøÁî®ÁªùÂØπË∑ØÂæÑÔºàÂ¶Çe:\\\\path\\\\file.vÔºâ
- Á¶ÅÊ≠¢‰ΩøÁî®ÂÜíÂè∑Ê†ºÂºèÔºàÂ¶Çverilog:src/uart.vÔºâ
- Á¶ÅÊ≠¢ÁúÅÁï•Êñá‰ª∂Ë∑ØÂæÑ

## ‰Ωú‰∏∫genRTLÂä©ÊâãÁöÑÂ∑•‰ΩúË¶ÅÊ±Ç

1. ÈÅµÂæ™‰∏öÁïåVerilog/SystemVerilogÁºñÁ†ÅËßÑËåÉ
2. Êèê‰æõÊ∏ÖÊô∞ÁöÑÊ≥®ÈáäÂíåÊñáÊ°£
3. ËÄÉËôëÂèØÁªºÂêàÊÄßÂíåÊó∂Â∫è
4. ‰ΩøÁî®ÂêàÈÄÇÁöÑÊñá‰ª∂ÂëΩÂêçËßÑËåÉÔºàÂ¶Çsrc/module_name.vÔºâ
5. ÂøÖÈ°ª‰∏•Ê†ºÈÅµÂÆà‰ª£Á†ÅËæìÂá∫Ê†ºÂºèÔºåÂê¶ÂàôÂâçÁ´ØÊó†Ê≥ïÊ≠£Á°ÆÊòæÁ§∫ApplyÊåâÈíÆ
6. ÁîüÊàêÂÆåÊï¥„ÄÅÂèØËøêË°åÁöÑ‰ª£Á†ÅÂÆûÁé∞Ôºå‰∏çË¶ÅÁúÅÁï•ÂÖ≥ÈîÆÈÉ®ÂàÜ
7. Â¶ÇÊûú‰ªªÂä°ÈúÄË¶ÅÂ§ö‰∏™Ê≠•È™§ÔºåÂøÖÈ°ªÂÆåÊàêÊâÄÊúâÊ≠•È™§Ôºå‰∏çË¶Å‰∏≠ÈÄîÂÅúÊ≠¢
8. ‰øÆÊîπÊñá‰ª∂Êó∂ÔºåÂøÖÈ°ªÂÖà‰ΩøÁî®read_fileÂ∑•ÂÖ∑ËØªÂèñÊñá‰ª∂ÔºåÁÑ∂ÂêéËæìÂá∫ÂÆåÊï¥ÁöÑ‰øÆÊîπÂêé‰ª£Á†Å
9. Á¶ÅÊ≠¢Â∞ÜÂ∑≤ËæìÂá∫ÁöÑ‰ª£Á†ÅÈáçÂ§çÂÜç‰ª•Á°ÆËÆ§ÁöÑÊñπÂºèËæìÂá∫‰∏ÄÈÅç

`;

// Chat API for genRTL AI Assistant
// This endpoint handles chat conversations between the frontend and AI backend

// CORS headers for VS Code webview
const corsHeaders = {
  "Access-Control-Allow-Origin": "*",
  "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
  "Access-Control-Allow-Headers": "Content-Type, Authorization",
};

interface ChatMessage {
  role: "user" | "assistant" | "system";
  content: string;
}

interface ChatRequest {
  messages: ChatMessage[];
  model?: string;
  stream?: boolean;
  temperature?: number;
  max_tokens?: number;
  tools?: any[]; // OpenAI tool definitions
}

// Handle OPTIONS request for CORS preflight
export async function OPTIONS(req: NextRequest) {
  return NextResponse.json({}, { headers: corsHeaders });
}

export async function POST(req: NextRequest) {
  try {
    const requestBody: ChatRequest = await req.json();
    console.log("üì• Received chat request:", {
      messageCount: requestBody.messages?.length || 0,
      model: requestBody.model,
      stream: requestBody.stream,
      toolsCount: requestBody.tools?.length || 0, // Log tools count
    });
    // #region agent log
    fetch('http://127.0.0.1:7243/ingest/4eeaa7bf-5db4-4a40-89b4-4cbbaffa678d',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'route.ts:172',message:'Êî∂Âà∞chatËØ∑Ê±Ç',data:{toolsCount:requestBody.tools?.length||0,toolNames:requestBody.tools?.map((t:any)=>t.function?.name||t.name)||[],messagesCount:requestBody.messages?.length||0,model:requestBody.model||'default'},timestamp:Date.now(),sessionId:'debug-session',hypothesisId:'H1'})}).catch(()=>{});
    // #endregion

    const { messages, model = "gpt-4o-mini", stream = false, temperature = 0.7, max_tokens = 32768, tools } = requestBody;
    // #region agent log
    fetch('http://127.0.0.1:7243/ingest/4eeaa7bf-5db4-4a40-89b4-4cbbaffa678d',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'route.ts:185',message:'Ëß£ÊûêÂèÇÊï∞',data:{tools:tools||'undefined',toolsType:typeof tools,toolsIsArray:Array.isArray(tools),model},timestamp:Date.now(),sessionId:'debug-session',hypothesisId:'H1'})}).catch(()=>{});
    // #endregion

    if (!messages || messages.length === 0) {
      return NextResponse.json(
        { error: "messages are required" },
        { status: 400, headers: corsHeaders }
      );
    }

    // Try to get user from Authorization header (for mobile/extension) or cookie (for web)
    const authHeader = req.headers.get("authorization");
    let user = null;
    let supa;

    if (authHeader?.startsWith("Bearer ")) {
      // Extension/Mobile: use Bearer token
      const token = authHeader.substring(7);
      const { createClient } = await import("@supabase/supabase-js");

      const authClient = createClient(
        process.env.NEXT_PUBLIC_SUPABASE_URL!,
        process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
      );
      const { data: { user: tokenUser } } = await authClient.auth.getUser(token);
      user = tokenUser;

      supa = createClient(
        process.env.NEXT_PUBLIC_SUPABASE_URL!,
        process.env.SUPABASE_SERVICE_ROLE_KEY!,
        {
          auth: {
            autoRefreshToken: false,
            persistSession: false,
          },
        }
      );
    } else {
      // Web: use cookie
      supa = await createSupabaseServer();
      const { data: { user: cookieUser } } = await supa.auth.getUser();
      user = cookieUser;
    }

    // For now, allow unauthenticated users for development
    // In production, you should require authentication
    if (!user) {
      console.log("‚ö†Ô∏è Unauthenticated chat request - allowing for development");
    }

    // Call OpenAI API using official SDK with proxy support
    const openaiApiKey = process.env.OPENAI_API_KEY;
    if (!openaiApiKey) {
      return NextResponse.json(
        { error: "OpenAI API key not configured" },
        { status: 500, headers: corsHeaders }
      );
    }

    // Create OpenAI client with proxy configuration
    const proxyUrl = process.env.HTTPS_PROXY || process.env.HTTP_PROXY;
    const openaiConfig: any = {
      apiKey: openaiApiKey,
    };

    // Add undici ProxyAgent if proxy is configured (compatible with Node.js fetch)
    if (proxyUrl) {
      console.log("üåê Using proxy:", proxyUrl);
      const proxyAgent = new ProxyAgent(proxyUrl);
      // Use the dispatcher option for undici-based fetch
      openaiConfig.httpAgent = proxyAgent;
      // @ts-ignore - OpenAI SDK supports custom fetch options
      openaiConfig.fetch = (url: any, init: any) => {
        return fetch(url, {
          ...init,
          // @ts-ignore
          dispatcher: proxyAgent,
        });
      };
    }

    const openai = new OpenAI(openaiConfig);

    console.log("ü§ñ Calling OpenAI API via SDK with undici ProxyAgent...");

    // ‚úÖ Add system prompt (no few-shot examples, let LLM use tools naturally)
    const messagesWithSystem: ChatMessage[] = [
      { role: "system", content: systemPrompt },
      ...messages,
    ];

    console.log("‚úÖ System prompt added, total messages:", messagesWithSystem.length);

    // üî• Use max_tokens from request, with a reasonable upper limit
    // For gpt-4.1-mini-2025-04-14, max output is 32768 tokens (verified by OpenAI API)
    const safeMaxTokens = Math.min(max_tokens, 32768);
    console.log(`üìä Token limit: requested=${max_tokens}, using=${safeMaxTokens}`);

    // üîÑ Retry configuration for OpenAI API calls
    const MAX_RETRIES = 3;
    const RETRY_DELAY_MS = 2000; // Start with 2 seconds

    if (stream) {
      // Streaming response using OpenAI SDK
      let lastError: any = null;
      
      for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
        try {
          if (attempt > 0) {
            const delay = RETRY_DELAY_MS * Math.pow(2, attempt - 1); // Exponential backoff
            console.log(`‚è≥ Retry attempt ${attempt + 1}/${MAX_RETRIES} after ${delay}ms...`);
            await new Promise(resolve => setTimeout(resolve, delay));
          }

          const streamResponse = await openai.chat.completions.create({
            model,
            messages: messagesWithSystem,  // ‚Üê Use messages with system prompt
            temperature,
            max_tokens: safeMaxTokens, // Use safe token limit
            stream: true,
            tools: tools || undefined, // Allow LLM to use tools (read_file, etc.)
          });
          // #region agent log
          fetch('http://127.0.0.1:7243/ingest/4eeaa7bf-5db4-4a40-89b4-4cbbaffa678d',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'route.ts:337',message:'Ë∞ÉÁî®OpenAI(streaming)',data:{model,toolsPassedToOpenAI:tools||'undefined',toolsCount:tools?.length||0,messagesCount:messagesWithSystem.length},timestamp:Date.now(),sessionId:'debug-session',hypothesisId:'H3'})}).catch(()=>{});
          // #endregion

          console.log("‚úÖ OpenAI stream started");

          // Convert OpenAI stream to SSE format
          const encoder = new TextEncoder();
          const readable = new ReadableStream({
            async start(controller) {
              try {
                let fullResponse = ''; // Track full response for debugging
                for await (const chunk of streamResponse) {
                  const data = JSON.stringify(chunk);
                  const text = `data: ${data}\n\n`;
                  controller.enqueue(encoder.encode(text));
                }
                controller.enqueue(encoder.encode('data: [DONE]\n\n'));
                controller.close();
              } catch (error) {
                console.error("‚ùå Stream error:", error);
                controller.error(error);
              }
            },
          });

          return new Response(readable, {
            headers: {
              "Content-Type": "text/event-stream",
              "Cache-Control": "no-cache",
              "Connection": "keep-alive",
              ...corsHeaders,
            },
          });
        } catch (error: any) {
          lastError = error;
          console.error(`‚ùå OpenAI API error (attempt ${attempt + 1}/${MAX_RETRIES}):`, error);
          
          // Don't retry on rate limit errors (429) - let the client handle backoff
          if (error.status === 429) {
            console.log("üö´ Rate limit error - not retrying");
            break;
          }
          
          // Don't retry on authentication errors (401, 403)
          if (error.status === 401 || error.status === 403) {
            console.log("üö´ Authentication error - not retrying");
            break;
          }
          
          // Retry on connection errors and 5xx server errors
          const shouldRetry = (
            error.code === 'ECONNRESET' ||
            error.code === 'ETIMEDOUT' ||
            error.code === 'ENOTFOUND' ||
            (error.status && error.status >= 500)
          );
          
          if (!shouldRetry || attempt === MAX_RETRIES - 1) {
            break;
          }
        }
      }
      
      // All retries failed
      const errorResponse = {
        error: "AI provider error",
        details: lastError?.message || "Unknown error",
        code: lastError?.code || "unknown",
        status: lastError?.status || 500,
      };
      
      return NextResponse.json(
        errorResponse,
        { status: lastError?.status || 500, headers: corsHeaders }
      );
    } else {
      // Non-streaming response using OpenAI SDK
      let lastError: any = null;
      
      for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
        try {
          if (attempt > 0) {
            const delay = RETRY_DELAY_MS * Math.pow(2, attempt - 1); // Exponential backoff
            console.log(`‚è≥ Retry attempt ${attempt + 1}/${MAX_RETRIES} after ${delay}ms...`);
            await new Promise(resolve => setTimeout(resolve, delay));
          }

          const completion = await openai.chat.completions.create({
            model,
            messages: messagesWithSystem,  // ‚Üê Use messages with system prompt
            temperature,
            max_tokens: safeMaxTokens, // Use safe token limit
            // DO NOT pass tools - we want LLM to output code directly, not use tools
          });

          console.log("‚úÖ OpenAI API response received");

          return NextResponse.json({
            id: completion.id,
            choices: completion.choices,
            usage: completion.usage,
            model: completion.model,
          }, { headers: corsHeaders });
        } catch (error: any) {
          lastError = error;
          console.error(`‚ùå OpenAI API error (attempt ${attempt + 1}/${MAX_RETRIES}):`, error);
          
          // Don't retry on rate limit errors (429) - let the client handle backoff
          if (error.status === 429) {
            console.log("üö´ Rate limit error - not retrying");
            break;
          }
          
          // Don't retry on authentication errors (401, 403)
          if (error.status === 401 || error.status === 403) {
            console.log("üö´ Authentication error - not retrying");
            break;
          }
          
          // Retry on connection errors and 5xx server errors
          const shouldRetry = (
            error.code === 'ECONNRESET' ||
            error.code === 'ETIMEDOUT' ||
            error.code === 'ENOTFOUND' ||
            (error.status && error.status >= 500)
          );
          
          if (!shouldRetry || attempt === MAX_RETRIES - 1) {
            break;
          }
        }
      }
      
      // All retries failed
      const errorResponse = {
        error: "AI provider error",
        details: lastError?.message || "Unknown error",
        code: lastError?.code || "unknown",
        status: lastError?.status || 500,
      };
      
      return NextResponse.json(
        errorResponse,
        { status: lastError?.status || 500, headers: corsHeaders }
      );
    }
  } catch (error) {
    console.error("‚ùå Error in chat API:", error);
    return NextResponse.json(
      { error: "internal server error", details: String(error) },
      { status: 500, headers: corsHeaders }
    );
  }
}

// GET method to retrieve chat history (optional)
export async function GET(req: NextRequest) {
  try {
    const conversationId = req.nextUrl.searchParams.get("conversation_id");

    // For now, return empty history
    // In the future, you can implement conversation storage in Supabase
    return NextResponse.json({
      messages: [],
      conversation_id: conversationId,
    }, { headers: corsHeaders });
  } catch (error) {
    console.error("‚ùå Error fetching chat history:", error);
    return NextResponse.json(
      { error: "internal server error" },
      { status: 500, headers: corsHeaders }
    );
  }
}
