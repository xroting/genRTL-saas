import { NextRequest, NextResponse } from "next/server";
import { createSupabaseServer } from "@/lib/supabase/server";
import OpenAI from "openai";
import { ProxyAgent } from "undici";

const systemPrompt = `ä½ æ˜¯ä¸€åä¼˜ç§€çš„FPGAå’ŒASICæ•°å­—å‰ç«¯é«˜çº§å·¥ç¨‹å¸ˆï¼Œç²¾é€šVerilog/SystemVerilog HDLè¯­è¨€è®¾è®¡ã€‚

## æ ¸å¿ƒå·¥ä½œæµç¨‹

å½“ç”¨æˆ·è¦æ±‚ä¿®æ”¹ç°æœ‰æ–‡ä»¶æ—¶ï¼š
1. å…ˆä½¿ç”¨read_fileå·¥å…·è¯»å–æ–‡ä»¶å†…å®¹ï¼Œäº†è§£å®Œæ•´ä¸Šä¸‹æ–‡
2. ç„¶åä½¿ç”¨SEARCH/REPLACEå—æ ¼å¼è¾“å‡ºä¿®æ”¹ï¼ˆåªè¾“å‡ºè¦æ”¹çš„éƒ¨åˆ†ï¼Œä¸è¦è¾“å‡ºæ•´ä¸ªæ–‡ä»¶ï¼‰
3. ä¸è¦çŒœæµ‹æ–‡ä»¶å†…å®¹ï¼Œå¿…é¡»å…ˆè¯»å–

å½“ç”¨æˆ·è¦æ±‚åˆ›å»ºæ–°æ–‡ä»¶æ—¶ï¼š
1. ç›´æ¥è¾“å‡ºå®Œæ•´ä»£ç 
2. ä½¿ç”¨åˆç†çš„æ–‡ä»¶è·¯å¾„ï¼ˆsrc/, rtl/, tb/ç­‰ï¼‰

## ä»£ç è¾“å‡ºæ ¼å¼

### æ ¼å¼Aï¼šåˆ›å»ºæ–°æ–‡ä»¶ï¼ˆè¾“å‡ºå®Œæ•´ä»£ç ï¼‰

ä½¿ç”¨æ ‡å‡†Markdownä»£ç å—ï¼š

ä¸‰ä¸ªåå¼•å· + è¯­è¨€å
æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
å®Œæ•´çš„ä»£ç å†…å®¹
ä¸‰ä¸ªåå¼•å·

### æ ¼å¼Bï¼šä¿®æ”¹ç°æœ‰æ–‡ä»¶ï¼ˆåªè¾“å‡ºè¦æ”¹çš„éƒ¨åˆ† - æ¨èï¼‰

ä½¿ç”¨SEARCH/REPLACEå—æ ¼å¼ï¼š

ä¸‰ä¸ªåå¼•å· + è¯­è¨€å
æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
<<<<<<< ORIGINAL
è¦æ›¿æ¢çš„åŸå§‹ä»£ç ï¼ˆåŒ…å«3-5è¡Œä¸Šä¸‹æ–‡ï¼‰
=======
ä¿®æ”¹åçš„ä»£ç 
>>>>>>> UPDATED
ä¸‰ä¸ªåå¼•å·

**å…³é”®è§„åˆ™ï¼š**
- ORIGINALå—å¿…é¡»åŒ…å«è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ï¼ˆä¿®æ”¹ç‚¹å‰åå„3-5è¡Œï¼‰ï¼Œç¡®ä¿å”¯ä¸€åŒ¹é…
- å¿…é¡»ç²¾ç¡®åŒ¹é…åŸæ–‡ä»¶å†…å®¹ï¼ˆåŒ…æ‹¬ç©ºæ ¼ã€ç¼©è¿›ï¼‰
- å¦‚æœä¸€ä¸ªæ–‡ä»¶æœ‰å¤šå¤„ä¿®æ”¹ä¸”ä¸ç›¸é‚»ï¼Œä½¿ç”¨å¤šä¸ªç‹¬ç«‹çš„ä»£ç å—
- ç¦æ­¢ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼ˆå¦‚e:\\\\path\\\\file.vï¼‰
- ç¦æ­¢ä½¿ç”¨å†’å·æ ¼å¼ï¼ˆå¦‚verilog:src/uart.vï¼‰

## ç¤ºä¾‹

ç”¨æˆ·ï¼š"è¯·å°†spi_masterçš„data_inä½å®½ä»8bitæ”¹ä¸º16bit"

ä½ çš„å›ç­”ï¼š

æˆ‘å°†ä¿®æ”¹spi_masteræ¨¡å—çš„è¾“å…¥æ•°æ®ä½å®½ï¼š

ä¸‰ä¸ªåå¼•å·verilog
src/spi_master.v
<<<<<<< ORIGINAL
module spi_master #(
    parameter CLOCK_DIV = 4
) (
    input wire clk,
    input wire reset,
    input wire [7:0] data_in,  // å½“å‰æ˜¯8ä½
    output wire mosi
);
=======
module spi_master #(
    parameter CLOCK_DIV = 4
) (
    input wire clk,
    input wire reset,
    input wire [15:0] data_in,  // æ”¹ä¸º16ä½
    output wire mosi
);
>>>>>>> UPDATED
ä¸‰ä¸ªåå¼•å·

ä½å®½å·²ä»8bitæ”¹ä¸º16bitã€‚

`;

// Chat API for genRTL AI Assistant
// This endpoint handles chat conversations between the frontend and AI backend

// CORS headers for VS Code webview
const corsHeaders = {
  "Access-Control-Allow-Origin": "*",
  "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
  "Access-Control-Allow-Headers": "Content-Type, Authorization",
};

interface ChatMessage {
  role: "user" | "assistant" | "system";
  content: string;
}

interface ChatRequest {
  messages: ChatMessage[];
  model?: string;
  stream?: boolean;
  temperature?: number;
  max_tokens?: number;
  tools?: any[]; // OpenAI tool definitions
}

// Handle OPTIONS request for CORS preflight
export async function OPTIONS(req: NextRequest) {
  return NextResponse.json({}, { headers: corsHeaders });
}

export async function POST(req: NextRequest) {
  try {
    const requestBody: ChatRequest = await req.json();
    console.log("ğŸ“¥ Received chat request:", {
      messageCount: requestBody.messages?.length || 0,
      model: requestBody.model,
      stream: requestBody.stream,
      toolsCount: requestBody.tools?.length || 0, // Log tools count
    });
    // #region agent log
    fetch('http://127.0.0.1:7243/ingest/4eeaa7bf-5db4-4a40-89b4-4cbbaffa678d',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'route.ts:172',message:'æ”¶åˆ°chatè¯·æ±‚',data:{toolsCount:requestBody.tools?.length||0,toolNames:requestBody.tools?.map((t:any)=>t.function?.name||t.name)||[],messagesCount:requestBody.messages?.length||0,model:requestBody.model||'default'},timestamp:Date.now(),sessionId:'debug-session',hypothesisId:'H1'})}).catch(()=>{});
    // #endregion

    const { messages, model = "gpt-4o-mini", stream = false, temperature = 0.7, max_tokens = 32768, tools } = requestBody;
    // #region agent log
    fetch('http://127.0.0.1:7243/ingest/4eeaa7bf-5db4-4a40-89b4-4cbbaffa678d',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'route.ts:185',message:'è§£æå‚æ•°',data:{tools:tools||'undefined',toolsType:typeof tools,toolsIsArray:Array.isArray(tools),model},timestamp:Date.now(),sessionId:'debug-session',hypothesisId:'H1'})}).catch(()=>{});
    // #endregion

    if (!messages || messages.length === 0) {
      return NextResponse.json(
        { error: "messages are required" },
        { status: 400, headers: corsHeaders }
      );
    }

    // Try to get user from Authorization header (for mobile/extension) or cookie (for web)
    const authHeader = req.headers.get("authorization");
    let user = null;
    let supa;

    if (authHeader?.startsWith("Bearer ")) {
      // Extension/Mobile: use Bearer token
      const token = authHeader.substring(7);
      const { createClient } = await import("@supabase/supabase-js");

      const authClient = createClient(
        process.env.NEXT_PUBLIC_SUPABASE_URL!,
        process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
      );
      const { data: { user: tokenUser } } = await authClient.auth.getUser(token);
      user = tokenUser;

      supa = createClient(
        process.env.NEXT_PUBLIC_SUPABASE_URL!,
        process.env.SUPABASE_SERVICE_ROLE_KEY!,
        {
          auth: {
            autoRefreshToken: false,
            persistSession: false,
          },
        }
      );
    } else {
      // Web: use cookie
      supa = await createSupabaseServer();
      const { data: { user: cookieUser } } = await supa.auth.getUser();
      user = cookieUser;
    }

    // For now, allow unauthenticated users for development
    // In production, you should require authentication
    if (!user) {
      console.log("âš ï¸ Unauthenticated chat request - allowing for development");
    }

    // Call OpenAI API using official SDK with proxy support
    const openaiApiKey = process.env.OPENAI_API_KEY;
    if (!openaiApiKey) {
      return NextResponse.json(
        { error: "OpenAI API key not configured" },
        { status: 500, headers: corsHeaders }
      );
    }

    // Create OpenAI client with proxy configuration
    const proxyUrl = process.env.HTTPS_PROXY || process.env.HTTP_PROXY;
    const openaiConfig: any = {
      apiKey: openaiApiKey,
    };

    // Add undici ProxyAgent if proxy is configured (compatible with Node.js fetch)
    if (proxyUrl) {
      console.log("ğŸŒ Using proxy:", proxyUrl);
      const proxyAgent = new ProxyAgent(proxyUrl);
      // Use the dispatcher option for undici-based fetch
      openaiConfig.httpAgent = proxyAgent;
      // @ts-ignore - OpenAI SDK supports custom fetch options
      openaiConfig.fetch = (url: any, init: any) => {
        return fetch(url, {
          ...init,
          // @ts-ignore
          dispatcher: proxyAgent,
        });
      };
    }

    const openai = new OpenAI(openaiConfig);

    console.log("ğŸ¤– Calling OpenAI API via SDK with undici ProxyAgent...");

    // âœ… Add system prompt (no few-shot examples, let LLM use tools naturally)
    const messagesWithSystem: ChatMessage[] = [
      { role: "system", content: systemPrompt },
      ...messages,
    ];

    console.log("âœ… System prompt added, total messages:", messagesWithSystem.length);

    // ğŸ”¥ Use max_tokens from request, with a reasonable upper limit
    // For gpt-4.1-mini-2025-04-14, max output is 32768 tokens (verified by OpenAI API)
    const safeMaxTokens = Math.min(max_tokens, 32768);
    console.log(`ğŸ“Š Token limit: requested=${max_tokens}, using=${safeMaxTokens}`);

    // ğŸ”„ Retry configuration for OpenAI API calls
    const MAX_RETRIES = 3;
    const RETRY_DELAY_MS = 2000; // Start with 2 seconds

    if (stream) {
      // Streaming response using OpenAI SDK
      let lastError: any = null;
      
      for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
        try {
          if (attempt > 0) {
            const delay = RETRY_DELAY_MS * Math.pow(2, attempt - 1); // Exponential backoff
            console.log(`â³ Retry attempt ${attempt + 1}/${MAX_RETRIES} after ${delay}ms...`);
            await new Promise(resolve => setTimeout(resolve, delay));
          }

          const streamResponse = await openai.chat.completions.create({
            model,
            messages: messagesWithSystem,  // â† Use messages with system prompt
            temperature,
            max_tokens: safeMaxTokens, // Use safe token limit
            stream: true,
            tools: tools || undefined, // Allow LLM to use tools (read_file, etc.)
          });
          // #region agent log
          fetch('http://127.0.0.1:7243/ingest/4eeaa7bf-5db4-4a40-89b4-4cbbaffa678d',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'route.ts:337',message:'è°ƒç”¨OpenAI(streaming)',data:{model,toolsPassedToOpenAI:tools||'undefined',toolsCount:tools?.length||0,messagesCount:messagesWithSystem.length},timestamp:Date.now(),sessionId:'debug-session',hypothesisId:'H3'})}).catch(()=>{});
          // #endregion

          console.log("âœ… OpenAI stream started");

          // Convert OpenAI stream to SSE format
          const encoder = new TextEncoder();
          const readable = new ReadableStream({
            async start(controller) {
              try {
                let fullResponse = ''; // Track full response for debugging
                for await (const chunk of streamResponse) {
                  const data = JSON.stringify(chunk);
                  const text = `data: ${data}\n\n`;
                  controller.enqueue(encoder.encode(text));
                }
                controller.enqueue(encoder.encode('data: [DONE]\n\n'));
                controller.close();
              } catch (error) {
                console.error("âŒ Stream error:", error);
                controller.error(error);
              }
            },
          });

          return new Response(readable, {
            headers: {
              "Content-Type": "text/event-stream",
              "Cache-Control": "no-cache",
              "Connection": "keep-alive",
              ...corsHeaders,
            },
          });
        } catch (error: any) {
          lastError = error;
          console.error(`âŒ OpenAI API error (attempt ${attempt + 1}/${MAX_RETRIES}):`, error);
          
          // Don't retry on rate limit errors (429) - let the client handle backoff
          if (error.status === 429) {
            console.log("ğŸš« Rate limit error - not retrying");
            break;
          }
          
          // Don't retry on authentication errors (401, 403)
          if (error.status === 401 || error.status === 403) {
            console.log("ğŸš« Authentication error - not retrying");
            break;
          }
          
          // Retry on connection errors and 5xx server errors
          const shouldRetry = (
            error.code === 'ECONNRESET' ||
            error.code === 'ETIMEDOUT' ||
            error.code === 'ENOTFOUND' ||
            (error.status && error.status >= 500)
          );
          
          if (!shouldRetry || attempt === MAX_RETRIES - 1) {
            break;
          }
        }
      }
      
      // All retries failed
      const errorResponse = {
        error: "AI provider error",
        details: lastError?.message || "Unknown error",
        code: lastError?.code || "unknown",
        status: lastError?.status || 500,
      };
      
      return NextResponse.json(
        errorResponse,
        { status: lastError?.status || 500, headers: corsHeaders }
      );
    } else {
      // Non-streaming response using OpenAI SDK
      let lastError: any = null;
      
      for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
        try {
          if (attempt > 0) {
            const delay = RETRY_DELAY_MS * Math.pow(2, attempt - 1); // Exponential backoff
            console.log(`â³ Retry attempt ${attempt + 1}/${MAX_RETRIES} after ${delay}ms...`);
            await new Promise(resolve => setTimeout(resolve, delay));
          }

          const completion = await openai.chat.completions.create({
            model,
            messages: messagesWithSystem,  // â† Use messages with system prompt
            temperature,
            max_tokens: safeMaxTokens, // Use safe token limit
            // DO NOT pass tools - we want LLM to output code directly, not use tools
          });

          console.log("âœ… OpenAI API response received");

          return NextResponse.json({
            id: completion.id,
            choices: completion.choices,
            usage: completion.usage,
            model: completion.model,
          }, { headers: corsHeaders });
        } catch (error: any) {
          lastError = error;
          console.error(`âŒ OpenAI API error (attempt ${attempt + 1}/${MAX_RETRIES}):`, error);
          
          // Don't retry on rate limit errors (429) - let the client handle backoff
          if (error.status === 429) {
            console.log("ğŸš« Rate limit error - not retrying");
            break;
          }
          
          // Don't retry on authentication errors (401, 403)
          if (error.status === 401 || error.status === 403) {
            console.log("ğŸš« Authentication error - not retrying");
            break;
          }
          
          // Retry on connection errors and 5xx server errors
          const shouldRetry = (
            error.code === 'ECONNRESET' ||
            error.code === 'ETIMEDOUT' ||
            error.code === 'ENOTFOUND' ||
            (error.status && error.status >= 500)
          );
          
          if (!shouldRetry || attempt === MAX_RETRIES - 1) {
            break;
          }
        }
      }
      
      // All retries failed
      const errorResponse = {
        error: "AI provider error",
        details: lastError?.message || "Unknown error",
        code: lastError?.code || "unknown",
        status: lastError?.status || 500,
      };
      
      return NextResponse.json(
        errorResponse,
        { status: lastError?.status || 500, headers: corsHeaders }
      );
    }
  } catch (error) {
    console.error("âŒ Error in chat API:", error);
    return NextResponse.json(
      { error: "internal server error", details: String(error) },
      { status: 500, headers: corsHeaders }
    );
  }
}

// GET method to retrieve chat history (optional)
export async function GET(req: NextRequest) {
  try {
    const conversationId = req.nextUrl.searchParams.get("conversation_id");

    // For now, return empty history
    // In the future, you can implement conversation storage in Supabase
    return NextResponse.json({
      messages: [],
      conversation_id: conversationId,
    }, { headers: corsHeaders });
  } catch (error) {
    console.error("âŒ Error fetching chat history:", error);
    return NextResponse.json(
      { error: "internal server error" },
      { status: 500, headers: corsHeaders }
    );
  }
}
