import { NextRequest, NextResponse } from "next/server";
import { createSupabaseServer } from "@/lib/supabase/server";
import OpenAI from "openai";
import { ProxyAgent } from "undici";

// ============================================================================
// ğŸ”§ æœåŠ¡ç«¯å›ºåŒ–çš„ System Promptï¼ˆä¸ä¾èµ–å‰ç«¯ä¼ å…¥ï¼‰
// ============================================================================
const systemPrompt = `ä½ æ˜¯FPGA/ASICæ•°å­—å‰ç«¯å·¥ç¨‹å¸ˆï¼Œç²¾é€šVerilog/SystemVerilogã€‚

## å·¥ä½œæ¨¡å¼

ä½ åœ¨Agentæ¨¡å¼ä¸‹å·¥ä½œã€‚æ¯æ¬¡å“åº”å¿…é¡»è°ƒç”¨ä¸€ä¸ªå·¥å…·ï¼š
- éœ€è¦ä¿¡æ¯ â†’ è°ƒç”¨ ls_dir / read_file / get_dir_tree
- éœ€è¦ä¿®æ”¹ â†’ è°ƒç”¨ edit_file
- ä»»åŠ¡å®Œæˆ â†’ è°ƒç”¨ finalizeï¼ˆè¿™æ˜¯å”¯ä¸€çš„ç»“æŸæ–¹å¼ï¼‰

## é‡è¦è§„åˆ™

1. æ¯æ¬¡å“åº”å¿…é¡»è°ƒç”¨ä¸”åªè°ƒç”¨ä¸€ä¸ªå·¥å…·
2. ç¦æ­¢è¾“å‡ºçº¯æ–‡æœ¬å“åº”ï¼ˆé™¤éé€šè¿‡ finalize å·¥å…·ï¼‰
3. ä»»åŠ¡å®Œæˆæ—¶å¿…é¡»è°ƒç”¨ finalize å·¥å…·ï¼Œä¸èƒ½ç›´æ¥è¾“å‡ºæ€»ç»“

## å·¥ä½œæµç¨‹

ç¤ºä¾‹ï¼šç”¨æˆ·è¯´"ç»™srcç›®å½•ä¸‹æ‰€æœ‰.væ–‡ä»¶æ·»åŠ æ³¨é‡Š"

æ­¥éª¤1: è°ƒç”¨ ls_dir è·å–æ–‡ä»¶åˆ—è¡¨
æ­¥éª¤2: è°ƒç”¨ read_file(ç¬¬1ä¸ªæ–‡ä»¶)
æ­¥éª¤3: è°ƒç”¨ edit_file(ç¬¬1ä¸ªæ–‡ä»¶)
æ­¥éª¤4: è°ƒç”¨ read_file(ç¬¬2ä¸ªæ–‡ä»¶)
æ­¥éª¤5: è°ƒç”¨ edit_file(ç¬¬2ä¸ªæ–‡ä»¶)
...
æœ€åæ­¥éª¤: è°ƒç”¨ finalize(summary="å·²å®ŒæˆNä¸ªæ–‡ä»¶çš„ä¿®æ”¹")

## edit_file æ ¼å¼

<<<<<<< ORIGINAL
ä»read_fileç²¾ç¡®å¤åˆ¶çš„åŸå§‹ä»£ç 
=======
ä¿®æ”¹åçš„ä»£ç 
>>>>>>> UPDATED

## å…³é”®

- ä»»åŠ¡å®Œæˆå¿…é¡»è°ƒç”¨ finalize å·¥å…·
- ä¸è¦è¾“å‡ºçº¯æ–‡æœ¬ï¼Œå¿…é¡»è°ƒç”¨å·¥å…·`;

// ============================================================================
// ğŸ”§ æœåŠ¡ç«¯å›ºåŒ–çš„ Tools Schemaï¼ˆä¸ä¾èµ–å‰ç«¯æ¯è½®ä¼ å…¥ï¼‰
// ============================================================================
const SERVER_TOOLS: OpenAI.Chat.Completions.ChatCompletionTool[] = [
  {
    type: "function",
    function: {
      name: "finalize",
      description: "ä»»åŠ¡å®Œæˆæ—¶è°ƒç”¨æ­¤å·¥å…·ã€‚è¿™æ˜¯ç»“æŸAgentå¾ªç¯çš„å”¯ä¸€æ–¹å¼ã€‚è°ƒç”¨åAgentå°†åœæ­¢å¹¶æ˜¾ç¤ºæ€»ç»“ã€‚",
      parameters: {
        type: "object",
        properties: {
          summary: {
            type: "string",
            description: "ä»»åŠ¡å®Œæˆçš„æ€»ç»“ï¼Œä¾‹å¦‚ï¼š'å·²å®Œæˆ5ä¸ªæ–‡ä»¶çš„ä¿®æ”¹ï¼šfile1.v, file2.v, ...'",
          },
          files_modified: {
            type: "array",
            items: { type: "string" },
            description: "ä¿®æ”¹çš„æ–‡ä»¶åˆ—è¡¨",
          },
          success: {
            type: "boolean",
            description: "ä»»åŠ¡æ˜¯å¦æˆåŠŸå®Œæˆ",
          },
        },
        required: ["summary", "success"],
      },
    },
  },
  // å…¶ä»–å·¥å…·ç”±å‰ç«¯ä¼ å…¥ï¼Œè¿™é‡Œåªå®šä¹‰ finalize
];

// Chat API for genRTL AI Assistant
// This endpoint handles chat conversations between the frontend and AI backend

// CORS headers for VS Code webview
const corsHeaders = {
  "Access-Control-Allow-Origin": "*",
  "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
  "Access-Control-Allow-Headers": "Content-Type, Authorization",
};

interface ChatMessage {
  role: "user" | "assistant" | "system";
  content: string;
}

interface ChatRequest {
  messages: ChatMessage[];
  model?: string;
  stream?: boolean;
  temperature?: number;
  max_tokens?: number;
  tools?: any[]; // OpenAI tool definitions
}

// Handle OPTIONS request for CORS preflight
export async function OPTIONS(req: NextRequest) {
  return NextResponse.json({}, { headers: corsHeaders });
}

export async function POST(req: NextRequest) {
  try {
    const requestBody: ChatRequest = await req.json();
    console.log("ğŸ“¥ Received chat request:", {
      messageCount: requestBody.messages?.length || 0,
      model: requestBody.model,
      stream: requestBody.stream,
      toolsCount: requestBody.tools?.length || 0, // Log tools count
    });
    // #region agent log
    fetch('http://127.0.0.1:7243/ingest/4eeaa7bf-5db4-4a40-89b4-4cbbaffa678d',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'route.ts:172',message:'æ”¶åˆ°chatè¯·æ±‚',data:{toolsCount:requestBody.tools?.length||0,toolNames:requestBody.tools?.map((t:any)=>t.function?.name||t.name)||[],messagesCount:requestBody.messages?.length||0,model:requestBody.model||'default'},timestamp:Date.now(),sessionId:'debug-session',hypothesisId:'H1'})}).catch(()=>{});
    // #endregion

    const { messages, model = "gpt-4.1", stream = false, temperature = 0.7, max_tokens = 32768, tools } = requestBody;
    // #region agent log
    fetch('http://127.0.0.1:7243/ingest/4eeaa7bf-5db4-4a40-89b4-4cbbaffa678d',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'route.ts:185',message:'è§£æå‚æ•°',data:{tools:tools||'undefined',toolsType:typeof tools,toolsIsArray:Array.isArray(tools),model},timestamp:Date.now(),sessionId:'debug-session',hypothesisId:'H1'})}).catch(()=>{});
    // #endregion

    if (!messages || messages.length === 0) {
      return NextResponse.json(
        { error: "messages are required" },
        { status: 400, headers: corsHeaders }
      );
    }

    // Try to get user from Authorization header (for mobile/extension) or cookie (for web)
    const authHeader = req.headers.get("authorization");
    let user = null;
    let supa;

    if (authHeader?.startsWith("Bearer ")) {
      // Extension/Mobile: use Bearer token
      const token = authHeader.substring(7);
      const { createClient } = await import("@supabase/supabase-js");

      const authClient = createClient(
        process.env.NEXT_PUBLIC_SUPABASE_URL!,
        process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
      );
      const { data: { user: tokenUser } } = await authClient.auth.getUser(token);
      user = tokenUser;

      supa = createClient(
        process.env.NEXT_PUBLIC_SUPABASE_URL!,
        process.env.SUPABASE_SERVICE_ROLE_KEY!,
        {
          auth: {
            autoRefreshToken: false,
            persistSession: false,
          },
        }
      );
    } else {
      // Web: use cookie
      supa = await createSupabaseServer();
      const { data: { user: cookieUser } } = await supa.auth.getUser();
      user = cookieUser;
    }

    // For now, allow unauthenticated users for development
    // In production, you should require authentication
    if (!user) {
      console.log("âš ï¸ Unauthenticated chat request - allowing for development");
    }

    // Call OpenAI API using official SDK with proxy support
    const openaiApiKey = process.env.OPENAI_API_KEY;
    if (!openaiApiKey) {
      return NextResponse.json(
        { error: "OpenAI API key not configured" },
        { status: 500, headers: corsHeaders }
      );
    }

    // Create OpenAI client with proxy configuration
    const proxyUrl = process.env.HTTPS_PROXY || process.env.HTTP_PROXY;
    const openaiConfig: any = {
      apiKey: openaiApiKey,
    };

    // Add undici ProxyAgent if proxy is configured (compatible with Node.js fetch)
    if (proxyUrl) {
      console.log("ğŸŒ Using proxy:", proxyUrl);
      const proxyAgent = new ProxyAgent(proxyUrl);
      // Use the dispatcher option for undici-based fetch
      openaiConfig.httpAgent = proxyAgent;
      // @ts-ignore - OpenAI SDK supports custom fetch options
      openaiConfig.fetch = (url: any, init: any) => {
        return fetch(url, {
          ...init,
          // @ts-ignore
          dispatcher: proxyAgent,
        });
      };
    }

    const openai = new OpenAI(openaiConfig);

    console.log("ğŸ¤– Calling OpenAI API via SDK with undici ProxyAgent...");

    // âœ… Add system prompt (no few-shot examples, let LLM use tools naturally)
    const messagesWithSystem: ChatMessage[] = [
      { role: "system", content: systemPrompt },
      ...messages,
    ];

    console.log("âœ… System prompt added, total messages:", messagesWithSystem.length);
    
    // #region agent log
    fetch('http://127.0.0.1:7243/ingest/4eeaa7bf-5db4-4a40-89b4-4cbbaffa678d',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'route.ts:222',message:'SystemPrompt content check',data:{promptLength:systemPrompt.length,hasAgentMode:systemPrompt.includes('Agentæ¨¡å¼'),hasToolPriority:systemPrompt.includes('å·¥å…·ä¼˜å…ˆ'),hasExampleWorkflow:systemPrompt.includes('å…¸å‹å·¥ä½œæµç¨‹ç¤ºä¾‹'),firstUserMessage:messages[messages.length-1]?.content?.substring(0,100)},timestamp:Date.now(),sessionId:'debug-session',hypothesisId:'A'})}).catch(()=>{});
    // #endregion

    // ğŸ”¥ Use max_tokens from request, with a reasonable upper limit
    // For gpt-4.1, max output is 32768 tokens, context window is ~1M tokens (verified by OpenAI API)
    const safeMaxTokens = Math.min(max_tokens, 32768);
    console.log(`ğŸ“Š Token limit: requested=${max_tokens}, using=${safeMaxTokens}`);

    // ğŸ”„ Retry configuration for OpenAI API calls
    const MAX_RETRIES = 3;
    const RETRY_DELAY_MS = 2000; // Start with 2 seconds

    if (stream) {
      // Streaming response using OpenAI SDK
      let lastError: any = null;
      
      for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
        try {
          if (attempt > 0) {
            const delay = RETRY_DELAY_MS * Math.pow(2, attempt - 1); // Exponential backoff
            console.log(`â³ Retry attempt ${attempt + 1}/${MAX_RETRIES} after ${delay}ms...`);
            await new Promise(resolve => setTimeout(resolve, delay));
          }

          // ğŸ¯ ç»“æ„æ€§ä¿®å¤ï¼š
          // 1. æœåŠ¡ç«¯å›ºåŒ–å·¥å…· schemaï¼ˆSERVER_TOOLS + å‰ç«¯å·¥å…·ï¼‰
          // 2. å…¨ç¨‹ tool_choice: "required"ï¼ˆæ¨¡å‹å¿…é¡»è°ƒç”¨å·¥å…·ï¼‰
          // 3. é€šè¿‡ finalize å·¥å…·ä½œä¸ºå”¯ä¸€çš„ç»ˆæ­¢ä¿¡å·
          
          // åˆå¹¶æœåŠ¡ç«¯å›ºåŒ–å·¥å…· + å‰ç«¯ä¼ å…¥å·¥å…·
          const mergedTools = [
            ...SERVER_TOOLS,
            ...(tools || []),
          ];
          
          console.log(`ğŸ“Š Sending request - serverTools: ${SERVER_TOOLS.length}, clientTools: ${tools?.length || 0}, merged: ${mergedTools.length}, messages: ${messagesWithSystem.length}`);

          const streamResponse = await openai.chat.completions.create({
            model,
            messages: messagesWithSystem,
            temperature: 0.1, // æ‰¹é‡ä»»åŠ¡ä½¿ç”¨ä½ temperature
            max_tokens: safeMaxTokens,
            stream: true,
            tools: mergedTools,
            tool_choice: "required", // ğŸ”¥ å…¨ç¨‹å¼ºåˆ¶å·¥å…·è°ƒç”¨ï¼Œé€šè¿‡ finalize ç»“æŸ
            parallel_tool_calls: false, // One tool at a time
            // #region agent log
            // Log: Testing tool_choice parameter to limit concurrent tool calls (Hypothesis E)
            // #endregion
          }).catch((error) => {
            // #region agent log
            fetch('http://127.0.0.1:7243/ingest/4eeaa7bf-5db4-4a40-89b4-4cbbaffa678d',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'route.ts:244',message:'OpenAI API Error',data:{errorMessage:error.message,errorCode:error.code,errorType:error.type,model},timestamp:Date.now(),sessionId:'debug-session',hypothesisId:'API_ERROR'})}).catch(()=>{});
            // #endregion
            console.error("âŒ OpenAI API Error:", error);
            throw error;
          });
          // #region agent log
          fetch('http://127.0.0.1:7243/ingest/4eeaa7bf-5db4-4a40-89b4-4cbbaffa678d',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'route.ts:260',message:'è°ƒç”¨OpenAI(required+finalize)',data:{model,mergedToolsCount:mergedTools.length,serverToolsCount:SERVER_TOOLS.length,clientToolsCount:tools?.length||0,messagesCount:messagesWithSystem.length,toolChoice:'required'},timestamp:Date.now(),sessionId:'debug-session',hypothesisId:'FINALIZE'})}).catch(()=>{});
          // #endregion

          console.log("âœ… OpenAI stream started");

          // Convert OpenAI stream to SSE format
          const encoder = new TextEncoder();
          const readable = new ReadableStream({
            async start(controller) {
              try {
                let fullResponse = ''; // Track full response for debugging
                let chunkCount = 0;
                let hasContent = false;
                let hasToolCalls = false;
                for await (const chunk of streamResponse) {
                  chunkCount++;
                  
                  // Track content and tool calls
                  if (chunk.choices[0]?.delta?.content) {
                    hasContent = true;
                  }
                  
                  // Log tool calls for debugging
                  if (chunk.choices[0]?.delta?.tool_calls) {
                    hasToolCalls = true;
                    console.log('ğŸ”§ Tool call chunk:', JSON.stringify(chunk.choices[0].delta.tool_calls, null, 2));
                    // #region agent log
                    fetch('http://127.0.0.1:7243/ingest/4eeaa7bf-5db4-4a40-89b4-4cbbaffa678d',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'route.ts:276',message:'Tool call detected',data:{chunkCount,toolCallsData:chunk.choices[0].delta.tool_calls,toolCallCount:chunk.choices[0].delta.tool_calls.length},timestamp:Date.now(),sessionId:'debug-session',hypothesisId:'C'})}).catch(()=>{});
                    // #endregion
                  }
                  
                  const data = JSON.stringify(chunk);
                  const text = `data: ${data}\n\n`;
                  controller.enqueue(encoder.encode(text));
                }
                
                // #region agent log
                fetch('http://127.0.0.1:7243/ingest/4eeaa7bf-5db4-4a40-89b4-4cbbaffa678d',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'route.ts:288',message:'Stream completed',data:{chunkCount,hasContent,hasToolCalls,model},timestamp:Date.now(),sessionId:'debug-session',hypothesisId:'STREAM_COMPLETE'})}).catch(()=>{});
                // #endregion
                
                console.log(`âœ… Stream completed, sent ${chunkCount} chunks, hasContent: ${hasContent}, hasToolCalls: ${hasToolCalls}`);
                controller.enqueue(encoder.encode('data: [DONE]\n\n'));
                controller.close();
              } catch (error) {
                console.error("âŒ Stream error:", error);
                controller.error(error);
              }
            },
          });

          return new Response(readable, {
            headers: {
              "Content-Type": "text/event-stream",
              "Cache-Control": "no-cache",
              "Connection": "keep-alive",
              ...corsHeaders,
            },
          });
        } catch (error: any) {
          lastError = error;
          console.error(`âŒ OpenAI API error (attempt ${attempt + 1}/${MAX_RETRIES}):`, error);
          
          // Don't retry on rate limit errors (429) - let the client handle backoff
          if (error.status === 429) {
            console.log("ğŸš« Rate limit error - not retrying");
            break;
          }
          
          // Don't retry on authentication errors (401, 403)
          if (error.status === 401 || error.status === 403) {
            console.log("ğŸš« Authentication error - not retrying");
            break;
          }
          
          // Retry on connection errors and 5xx server errors
          const shouldRetry = (
            error.code === 'ECONNRESET' ||
            error.code === 'ETIMEDOUT' ||
            error.code === 'ENOTFOUND' ||
            (error.status && error.status >= 500)
          );
          
          if (!shouldRetry || attempt === MAX_RETRIES - 1) {
            break;
          }
        }
      }
      
      // All retries failed
      const errorResponse = {
        error: "AI provider error",
        details: lastError?.message || "Unknown error",
        code: lastError?.code || "unknown",
        status: lastError?.status || 500,
      };
      
      return NextResponse.json(
        errorResponse,
        { status: lastError?.status || 500, headers: corsHeaders }
      );
    } else {
      // Non-streaming response using OpenAI SDK
      let lastError: any = null;
      
      for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
        try {
          if (attempt > 0) {
            const delay = RETRY_DELAY_MS * Math.pow(2, attempt - 1); // Exponential backoff
            console.log(`â³ Retry attempt ${attempt + 1}/${MAX_RETRIES} after ${delay}ms...`);
            await new Promise(resolve => setTimeout(resolve, delay));
          }

          const completion = await openai.chat.completions.create({
            model,
            messages: messagesWithSystem,  // â† Use messages with system prompt
            temperature,
            max_tokens: safeMaxTokens, // Use safe token limit
            // DO NOT pass tools - we want LLM to output code directly, not use tools
          });

          console.log("âœ… OpenAI API response received");

          return NextResponse.json({
            id: completion.id,
            choices: completion.choices,
            usage: completion.usage,
            model: completion.model,
          }, { headers: corsHeaders });
        } catch (error: any) {
          lastError = error;
          console.error(`âŒ OpenAI API error (attempt ${attempt + 1}/${MAX_RETRIES}):`, error);
          
          // Don't retry on rate limit errors (429) - let the client handle backoff
          if (error.status === 429) {
            console.log("ğŸš« Rate limit error - not retrying");
            break;
          }
          
          // Don't retry on authentication errors (401, 403)
          if (error.status === 401 || error.status === 403) {
            console.log("ğŸš« Authentication error - not retrying");
            break;
          }
          
          // Retry on connection errors and 5xx server errors
          const shouldRetry = (
            error.code === 'ECONNRESET' ||
            error.code === 'ETIMEDOUT' ||
            error.code === 'ENOTFOUND' ||
            (error.status && error.status >= 500)
          );
          
          if (!shouldRetry || attempt === MAX_RETRIES - 1) {
            break;
          }
        }
      }
      
      // All retries failed
      const errorResponse = {
        error: "AI provider error",
        details: lastError?.message || "Unknown error",
        code: lastError?.code || "unknown",
        status: lastError?.status || 500,
      };
      
      return NextResponse.json(
        errorResponse,
        { status: lastError?.status || 500, headers: corsHeaders }
      );
    }
  } catch (error) {
    console.error("âŒ Error in chat API:", error);
    return NextResponse.json(
      { error: "internal server error", details: String(error) },
      { status: 500, headers: corsHeaders }
    );
  }
}

// GET method to retrieve chat history (optional)
export async function GET(req: NextRequest) {
  try {
    const conversationId = req.nextUrl.searchParams.get("conversation_id");

    // For now, return empty history
    // In the future, you can implement conversation storage in Supabase
    return NextResponse.json({
      messages: [],
      conversation_id: conversationId,
    }, { headers: corsHeaders });
  } catch (error) {
    console.error("âŒ Error fetching chat history:", error);
    return NextResponse.json(
      { error: "internal server error" },
      { status: 500, headers: corsHeaders }
    );
  }
}
