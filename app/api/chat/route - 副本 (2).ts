import { NextRequest, NextResponse } from "next/server";
import { createSupabaseServer } from "@/lib/supabase/server";
import OpenAI from "openai";
import { ProxyAgent } from "undici";

const systemPrompt = `ä½ æ˜¯ä¸€åä¼˜ç§€çš„FPGAå’ŒASICæ•°å­—å‰ç«¯é«˜çº§å·¥ç¨‹å¸ˆï¼Œå…·å¤‡ä»¥ä¸‹æŠ€èƒ½ï¼š
1ã€ç²¾é€šVerilog/SystemVerilog HDLè¯­è¨€ï¼Œèƒ½å¤Ÿç†Ÿç»ƒä½¿ç”¨Verilog/SystemVerilog HDLè¯­è¨€è¿›è¡Œæ•°å­—ç”µè·¯è®¾è®¡ï¼›
2ã€ç²¾é€šæ•°å­—å‰ç«¯è®¾è®¡ä¸­è·¨å¼‚æ­¥æ—¶é’Ÿå¤„ç†ï¼ŒçŠ¶æ€æœºï¼Œæµæ°´çº¿pipe-lineè®¾è®¡ï¼Œä¹’ä¹“æ“ä½œç­‰å¸¸è§çš„æ•°å­—å‰ç«¯è®¾è®¡æŠ€å·§ï¼›
3ã€ç²¾é€šæ•°å­—å‰ç«¯è®¾è®¡ä¸­å¸¸è§çš„éªŒè¯æ–¹æ³•ï¼Œå¦‚UVMï¼ŒSVï¼ŒC++ç­‰ï¼›
4ã€ç†Ÿæ‚‰Xilinx/Altera FPGAå„FPGAå‹å·çš„å„ç§èµ„æºï¼ˆå¦‚CLBï¼ŒBRAMï¼ŒDSPï¼ŒSerdesï¼ŒIOç­‰ï¼‰ï¼Œèƒ½å¤Ÿæ ¹æ®éœ€æ±‚åˆç†åˆ†é…èµ„æºï¼›
5ã€ç†Ÿæ‚‰Xilinx/Altera FPGAå„FPGAå‹å·çš„æ—¶é’Ÿçº¦æŸï¼Œèƒ½å¤Ÿæ ¹æ®éœ€æ±‚åˆç†è®¾ç½®æ—¶é’Ÿçº¦æŸï¼›

ä½ çš„å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š
1ã€æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚åˆ—å‡ºè¦è®¾è®¡çš„ç”µè·¯éœ€æ±‚è§„æ ¼ï¼›
2ã€æ ¹æ®éœ€æ±‚è§„æ ¼ç»™å‡ºç”µè·¯çš„æ€»ä½“è®¾è®¡æ–¹æ¡ˆï¼šåˆ†å‡ ä¸ªä¸€çº§æ¨¡å—ï¼Œå®šä¹‰æ¯ä¸ªä¸€çº§æ¨¡å—åŠŸèƒ½ä¸€çº§æ¯ä¸ªä¸€çº§æ¨¡å—ä¹‹é—´è¾“å…¥è¾“å‡ºæ¥å£ï¼›
3ã€æ ¹æ®æ€»ä½“è®¾è®¡æ–¹æ¡ˆç»™å‡ºæ¯ä¸ªä¸€çº§æ¨¡å—çš„è¯¦ç»†è®¾è®¡æ–¹æ¡ˆï¼›
4ã€æ ¹æ®è¯¦ç»†è®¾è®¡æ–¹æ¡ˆè¿›è¡Œcodingï¼Œç”ŸæˆVerilog/SystemVerilog RTLä»£ç ï¼›
5ã€æ ¹æ®éœ€æ±‚è§„æ ¼å’Œæ€»ä½“æ–¹æ¡ˆè®¾è®¡éªŒè¯æ–¹æ¡ˆï¼Œè®¾è®¡éªŒè¯ç¯å¢ƒï¼Œç¼–å†™éªŒè¯ç¯å¢ƒä»£ç å’Œæµ‹è¯•ç”¨ä¾‹è„šæœ¬ï¼›
6ã€è¿è¡Œæµ‹è¯•ç”¨ä¾‹ï¼Œæ ¹æ®æµ‹è¯•è¾“å‡ºè¿­ä»£ä¼˜åŒ–RTLä»£ç 
## â€¼ï¸ é‡è¦ï¼šä»£ç è¾“å‡ºæ ¼å¼è§„èŒƒï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

### ğŸ“ åˆ›å»ºæ–°æ–‡ä»¶æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š

\`\`\`language:path/to/filename.ext
ä»£ç å†…å®¹
\`\`\`

**ç¤ºä¾‹ï¼š**
\`\`\`verilog:src/uart.v
module uart(
  input wire clk,
  input wire [7:0] data,
  output reg tx
);
  // Implementation
endmodule
\`\`\`

## âš ï¸ å…³é”®è§„åˆ™

1. **æ€»æ˜¯åŒ…å«æ–‡ä»¶å**ï¼šå³ä½¿ç”¨æˆ·æ²¡æœ‰æ˜ç¡®è¦æ±‚ï¼Œä¹Ÿè¦è‡ªåŠ¨ç”Ÿæˆåˆç†çš„æ–‡ä»¶å
2. **ä½¿ç”¨æ­£ç¡®çš„è¯­è¨€æ ‡è¯†ç¬¦**ï¼šverilog, systemverilog, python, javascriptç­‰
3. **æ–‡ä»¶è·¯å¾„è¦åˆç†**ï¼šé€šå¸¸æ”¾åœ¨ src/, rtl/, tb/ ç­‰ç›®å½•
4. **ä¿®æ”¹æ—¶åŒ…å«è¡Œå·**ï¼šå¦‚æœæ˜¯ä¿®æ”¹ç°æœ‰æ–‡ä»¶ï¼Œå¿…é¡»æŒ‡å®šè¡Œå·èŒƒå›´

## âŒ é”™è¯¯æ ¼å¼ï¼ˆç»å¯¹ä¸è¦ä½¿ç”¨ï¼‰

\`\`\`verilog          â† é”™è¯¯ï¼šç¼ºå°‘æ–‡ä»¶å
\`\`\`verilog src/uart.v  â† é”™è¯¯ï¼šç¼ºå°‘å†’å·
\`\`\`verilog: src/uart.v â† é”™è¯¯ï¼šå†’å·åæœ‰ç©ºæ ¼

## âœ… æ­£ç¡®æ ¼å¼

\`\`\`verilog:src/uart.v  â† æ­£ç¡®

## ğŸ“‹ å“åº”ç»“æ„ç¤ºä¾‹

ç”¨æˆ·: "è¯·ç”¨verilogå†™ä¸€ä¸ªUARTç”µè·¯ï¼Œè¦æ±‚8bitæ•°æ®ä½"

ä½ çš„å›ç­”åº”è¯¥è¿™æ ·ï¼š
"""
æˆ‘æ¥åˆ›å»ºä¸€ä¸ªUARTæ¨¡å—ï¼Œæ”¯æŒ8ä½æ•°æ®ä¼ è¾“ï¼š

\`\`\`verilog:src/uart.v
module uart (
  input wire clk,
  input wire reset,
  input wire [7:0] tx_data,
  input wire tx_start,
  output reg tx,
  output wire tx_busy,
  input wire rx,
  output reg [7:0] rx_data,
  output reg rx_valid
);

parameter BAUD_RATE = 115200;
parameter CLOCK_FREQ = 50000000;

// Implementation here...

endmodule
\`\`\`

è¿™ä¸ªUARTæ¨¡å—å®ç°äº†ï¼š
1. 8ä½æ•°æ®ä¼ è¾“
2. å¯é…ç½®æ³¢ç‰¹ç‡
3. å‘é€å’Œæ¥æ”¶åŠŸèƒ½
"""

## æ”¯æŒçš„è¯­è¨€æ ‡è¯†ç¬¦

### HDLè¯­è¨€ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
- verilog - Verilog HDL
- systemverilog - SystemVerilog
- vhdl - VHDL

### å…¶ä»–è¯­è¨€
- python, c, cpp, 
- json, yaml, toml, xml
- bash, sh, powershell

## ä½œä¸ºgenRTLåŠ©æ‰‹ï¼Œä½ åº”è¯¥ï¼š
1. éµå¾ªä¸šç•ŒVerilog/SystemVerilogç¼–ç è§„èŒƒ
2. æä¾›æ¸…æ™°çš„æ³¨é‡Šå’Œæ–‡æ¡£
3. è€ƒè™‘å¯ç»¼åˆæ€§å’Œæ—¶åº
4. ä½¿ç”¨åˆé€‚çš„æ–‡ä»¶å‘½åè§„èŒƒ
5. **å¿…é¡»ä¸¥æ ¼éµå®ˆä»£ç è¾“å‡ºæ ¼å¼ï¼Œå¦åˆ™å‰ç«¯æ— æ³•æ­£ç¡®æ˜¾ç¤º**
6. **ç”Ÿæˆå®Œæ•´ã€å¯è¿è¡Œçš„ä»£ç å®ç°ï¼Œä¸è¦çœç•¥å…³é”®éƒ¨åˆ†ï¼ŒåŒ…æ‹¬æ‰€æœ‰é€»è¾‘å’ŒçŠ¶æ€æœº**
7. **å¦‚æœä»»åŠ¡éœ€è¦å¤šä¸ªæ­¥éª¤ï¼Œå¿…é¡»å®Œæˆæ‰€æœ‰æ­¥éª¤ï¼Œä¸è¦ä¸­é€”åœæ­¢**


## ğŸ’¡ ä»£ç å®Œæ•´æ€§è¦æ±‚

âŒ **ä¸è¦è¿™æ ·åš**ï¼ˆä¸å®Œæ•´ï¼‰ï¼š
\`\`\`verilog:src/uart.v
module uart(...);
  // Implementation here...  â† è¿™æ˜¯ä¸å®Œæ•´çš„ï¼
endmodule
\`\`\`

âœ… **å¿…é¡»è¿™æ ·åš**ï¼ˆå®Œæ•´å®ç°ï¼‰ï¼š
\`\`\`verilog:src/uart.v
module uart(
  input wire clk,
  input wire reset,
  input wire [7:0] tx_data,
  input wire tx_start,
  output reg tx,
  output wire tx_busy
);

parameter BAUD_RATE = 115200;
parameter CLOCK_FREQ = 50000000;
localparam BAUD_DIVISOR = CLOCK_FREQ / BAUD_RATE;

// å®Œæ•´çš„å‘é€çŠ¶æ€æœº
reg [2:0] tx_state;
reg [15:0] baud_counter;
reg [3:0] bit_counter;
// ... æ‰€æœ‰å¿…è¦çš„å¯„å­˜å™¨å’Œé€»è¾‘

always @(posedge clk or posedge reset) begin
  if (reset) begin
    // å®Œæ•´çš„å¤ä½é€»è¾‘
  end else begin
    // å®Œæ•´çš„çŠ¶æ€æœºå®ç°
    case (tx_state)
      // æ‰€æœ‰çŠ¶æ€çš„å®Œæ•´å®ç°
    endcase
  end
end

endmodule
\`\`\`
**å…³é”®ï¼šç”¨æˆ·è¦æ±‚å®ç°åŠŸèƒ½æ—¶ï¼Œå¿…é¡»æä¾›å®Œæ•´ã€å¯ç»¼åˆã€å¯æµ‹è¯•çš„ä»£ç ï¼Œä¸è¦ç•™ç©ºæˆ–çœç•¥ï¼**

ç»å¯¹ç¦æ­¢è¿™æ ·åšï¼šç¦æ­¢å°†å·²è¾“å‡ºæˆ–ä¿®æ”¹çš„ä»£ç é‡å¤å†ä»¥ç¡®è®¤çš„æ–¹å¼è¾“å‡ºä¸€éã€‚

`;

// Chat API for genRTL AI Assistant
// This endpoint handles chat conversations between the frontend and AI backend

// CORS headers for VS Code webview
const corsHeaders = {
  "Access-Control-Allow-Origin": "*",
  "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
  "Access-Control-Allow-Headers": "Content-Type, Authorization",
};

interface ChatMessage {
  role: "user" | "assistant" | "system";
  content: string;
}

interface ChatRequest {
  messages: ChatMessage[];
  model?: string;
  stream?: boolean;
  temperature?: number;
  max_tokens?: number;
  tools?: any[]; // OpenAI tool definitions
}

// Handle OPTIONS request for CORS preflight
export async function OPTIONS(req: NextRequest) {
  return NextResponse.json({}, { headers: corsHeaders });
}

export async function POST(req: NextRequest) {
  try {
    const requestBody: ChatRequest = await req.json();
    console.log("ğŸ“¥ Received chat request:", {
      messageCount: requestBody.messages?.length || 0,
      model: requestBody.model,
      stream: requestBody.stream,
      toolsCount: requestBody.tools?.length || 0, // Log tools count
    });

    const { messages, model = "gpt-4o-mini", stream = false, temperature = 0.7, max_tokens = 32768, tools } = requestBody;

    if (!messages || messages.length === 0) {
      return NextResponse.json(
        { error: "messages are required" },
        { status: 400, headers: corsHeaders }
      );
    }

    // Try to get user from Authorization header (for mobile/extension) or cookie (for web)
    const authHeader = req.headers.get("authorization");
    let user = null;
    let supa;

    if (authHeader?.startsWith("Bearer ")) {
      // Extension/Mobile: use Bearer token
      const token = authHeader.substring(7);
      const { createClient } = await import("@supabase/supabase-js");

      const authClient = createClient(
        process.env.NEXT_PUBLIC_SUPABASE_URL!,
        process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
      );
      const { data: { user: tokenUser } } = await authClient.auth.getUser(token);
      user = tokenUser;

      supa = createClient(
        process.env.NEXT_PUBLIC_SUPABASE_URL!,
        process.env.SUPABASE_SERVICE_ROLE_KEY!,
        {
          auth: {
            autoRefreshToken: false,
            persistSession: false,
          },
        }
      );
    } else {
      // Web: use cookie
      supa = await createSupabaseServer();
      const { data: { user: cookieUser } } = await supa.auth.getUser();
      user = cookieUser;
    }

    // For now, allow unauthenticated users for development
    // In production, you should require authentication
    if (!user) {
      console.log("âš ï¸ Unauthenticated chat request - allowing for development");
    }

    // Call OpenAI API using official SDK with proxy support
    const openaiApiKey = process.env.OPENAI_API_KEY;
    if (!openaiApiKey) {
      return NextResponse.json(
        { error: "OpenAI API key not configured" },
        { status: 500, headers: corsHeaders }
      );
    }

    // Create OpenAI client with proxy configuration
    const proxyUrl = process.env.HTTPS_PROXY || process.env.HTTP_PROXY;
    const openaiConfig: any = {
      apiKey: openaiApiKey,
    };

    // Add undici ProxyAgent if proxy is configured (compatible with Node.js fetch)
    if (proxyUrl) {
      console.log("ğŸŒ Using proxy:", proxyUrl);
      const proxyAgent = new ProxyAgent(proxyUrl);
      // Use the dispatcher option for undici-based fetch
      openaiConfig.httpAgent = proxyAgent;
      // @ts-ignore - OpenAI SDK supports custom fetch options
      openaiConfig.fetch = (url: any, init: any) => {
        return fetch(url, {
          ...init,
          // @ts-ignore
          dispatcher: proxyAgent,
        });
      };
    }

    const openai = new OpenAI(openaiConfig);

    console.log("ğŸ¤– Calling OpenAI API via SDK with undici ProxyAgent...");

    // âœ… Add system prompt and few-shot examples
    // ğŸš¨ CRITICAL: Few-shot examples must use RELATIVE paths (src/file.v) NOT absolute paths
    const messagesWithSystem: ChatMessage[] = [
      { role: "system", content: systemPrompt },
      // Few-shot example 1: Create new UART file - ä½¿ç”¨ç›¸å¯¹è·¯å¾„ src/uart.v
      { role: "user", content: "è¯·å¸®æˆ‘åˆ›å»ºä¸€ä¸ªUARTæ¨¡å—" },
      { role: "assistant", content: "å¥½çš„ï¼Œæˆ‘å°†ä¸ºæ‚¨åˆ›å»ºä¸€ä¸ªUARTæ¨¡å—ã€‚ä»¥ä¸‹æ˜¯ä»£ç ï¼š\n\n```verilog\nsrc/uart.v\nmodule uart(\n  input wire clk,\n  input wire reset,\n  output reg tx\n);\n\nparameter BAUD_RATE = 115200;\nparameter CLK_FREQ = 100000000;\n\nlocalparam DIVISOR = CLK_FREQ / BAUD_RATE;\nreg [15:0] counter;\n\nalways @(posedge clk) begin\n  if (reset) counter <= 0;\n  else counter <= counter + 1;\nend\n\nendmodule\n```\n\nè¿™ä¸ªUARTæ¨¡å—å·²ç»åŒ…å«äº†åŸºæœ¬çš„æ³¢ç‰¹ç‡åˆ†é¢‘å™¨ã€‚" },
      // Few-shot example 2: Modify existing file - è¾“å‡ºå®Œæ•´ä»£ç ï¼Œä½¿ç”¨ç›¸å¯¹è·¯å¾„
      { role: "user", content: "è¯·å°†æ—¶é’Ÿé¢‘ç‡æ”¹ä¸º50MHz" },
      { role: "assistant", content: "å¥½çš„ï¼Œæˆ‘å°†æ›´æ–°UARTæ¨¡å—çš„æ—¶é’Ÿé¢‘ç‡ä¸º50MHzã€‚ä»¥ä¸‹æ˜¯ä¿®æ”¹åçš„å®Œæ•´ä»£ç ï¼š\n\n```verilog\nsrc/uart.v\nmodule uart(\n  input wire clk,\n  input wire reset,\n  output reg tx\n);\n\nparameter BAUD_RATE = 115200;\nparameter CLK_FREQ = 50000000;  // å·²æ›´æ–°ä¸º50MHz\n\nlocalparam DIVISOR = CLK_FREQ / BAUD_RATE;\nreg [15:0] counter;\n\nalways @(posedge clk) begin\n  if (reset) counter <= 0;\n  else counter <= counter + 1;\nend\n\nendmodule\n```\n\næ—¶é’Ÿé¢‘ç‡å·²æ›´æ–°ä¸º50MHzã€‚" },
      ...messages,
    ];

    console.log("âœ… System prompt added, total messages:", messagesWithSystem.length);

    // ğŸ”¥ Use max_tokens from request, with a reasonable upper limit
    // For gpt-4.1-mini-2025-04-14, max output is 32768 tokens (verified by OpenAI API)
    const safeMaxTokens = Math.min(max_tokens, 32768);
    console.log(`ğŸ“Š Token limit: requested=${max_tokens}, using=${safeMaxTokens}`);

    // ğŸ”„ Retry configuration for OpenAI API calls
    const MAX_RETRIES = 3;
    const RETRY_DELAY_MS = 2000; // Start with 2 seconds

    if (stream) {
      // Streaming response using OpenAI SDK
      let lastError: any = null;
      
      for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
        try {
          if (attempt > 0) {
            const delay = RETRY_DELAY_MS * Math.pow(2, attempt - 1); // Exponential backoff
            console.log(`â³ Retry attempt ${attempt + 1}/${MAX_RETRIES} after ${delay}ms...`);
            await new Promise(resolve => setTimeout(resolve, delay));
          }

          const streamResponse = await openai.chat.completions.create({
            model,
            messages: messagesWithSystem,  // â† Use messages with system prompt
            temperature,
            max_tokens: safeMaxTokens, // Use safe token limit
            stream: true,
            // DO NOT pass tools - we want LLM to output code directly, not use tools
          });

          console.log("âœ… OpenAI stream started");

          // Convert OpenAI stream to SSE format
          const encoder = new TextEncoder();
          const readable = new ReadableStream({
            async start(controller) {
              try {
                let fullResponse = ''; // Track full response for debugging
                for await (const chunk of streamResponse) {
                  const data = JSON.stringify(chunk);
                  const text = `data: ${data}\n\n`;
                  controller.enqueue(encoder.encode(text));
                }
                controller.enqueue(encoder.encode('data: [DONE]\n\n'));
                controller.close();
              } catch (error) {
                console.error("âŒ Stream error:", error);
                controller.error(error);
              }
            },
          });

          return new Response(readable, {
            headers: {
              "Content-Type": "text/event-stream",
              "Cache-Control": "no-cache",
              "Connection": "keep-alive",
              ...corsHeaders,
            },
          });
        } catch (error: any) {
          lastError = error;
          console.error(`âŒ OpenAI API error (attempt ${attempt + 1}/${MAX_RETRIES}):`, error);
          
          // Don't retry on rate limit errors (429) - let the client handle backoff
          if (error.status === 429) {
            console.log("ğŸš« Rate limit error - not retrying");
            break;
          }
          
          // Don't retry on authentication errors (401, 403)
          if (error.status === 401 || error.status === 403) {
            console.log("ğŸš« Authentication error - not retrying");
            break;
          }
          
          // Retry on connection errors and 5xx server errors
          const shouldRetry = (
            error.code === 'ECONNRESET' ||
            error.code === 'ETIMEDOUT' ||
            error.code === 'ENOTFOUND' ||
            (error.status && error.status >= 500)
          );
          
          if (!shouldRetry || attempt === MAX_RETRIES - 1) {
            break;
          }
        }
      }
      
      // All retries failed
      const errorResponse = {
        error: "AI provider error",
        details: lastError?.message || "Unknown error",
        code: lastError?.code || "unknown",
        status: lastError?.status || 500,
      };
      
      return NextResponse.json(
        errorResponse,
        { status: lastError?.status || 500, headers: corsHeaders }
      );
    } else {
      // Non-streaming response using OpenAI SDK
      let lastError: any = null;
      
      for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
        try {
          if (attempt > 0) {
            const delay = RETRY_DELAY_MS * Math.pow(2, attempt - 1); // Exponential backoff
            console.log(`â³ Retry attempt ${attempt + 1}/${MAX_RETRIES} after ${delay}ms...`);
            await new Promise(resolve => setTimeout(resolve, delay));
          }

          const completion = await openai.chat.completions.create({
            model,
            messages: messagesWithSystem,  // â† Use messages with system prompt
            temperature,
            max_tokens: safeMaxTokens, // Use safe token limit
            // DO NOT pass tools - we want LLM to output code directly, not use tools
          });

          console.log("âœ… OpenAI API response received");

          return NextResponse.json({
            id: completion.id,
            choices: completion.choices,
            usage: completion.usage,
            model: completion.model,
          }, { headers: corsHeaders });
        } catch (error: any) {
          lastError = error;
          console.error(`âŒ OpenAI API error (attempt ${attempt + 1}/${MAX_RETRIES}):`, error);
          
          // Don't retry on rate limit errors (429) - let the client handle backoff
          if (error.status === 429) {
            console.log("ğŸš« Rate limit error - not retrying");
            break;
          }
          
          // Don't retry on authentication errors (401, 403)
          if (error.status === 401 || error.status === 403) {
            console.log("ğŸš« Authentication error - not retrying");
            break;
          }
          
          // Retry on connection errors and 5xx server errors
          const shouldRetry = (
            error.code === 'ECONNRESET' ||
            error.code === 'ETIMEDOUT' ||
            error.code === 'ENOTFOUND' ||
            (error.status && error.status >= 500)
          );
          
          if (!shouldRetry || attempt === MAX_RETRIES - 1) {
            break;
          }
        }
      }
      
      // All retries failed
      const errorResponse = {
        error: "AI provider error",
        details: lastError?.message || "Unknown error",
        code: lastError?.code || "unknown",
        status: lastError?.status || 500,
      };
      
      return NextResponse.json(
        errorResponse,
        { status: lastError?.status || 500, headers: corsHeaders }
      );
    }
  } catch (error) {
    console.error("âŒ Error in chat API:", error);
    return NextResponse.json(
      { error: "internal server error", details: String(error) },
      { status: 500, headers: corsHeaders }
    );
  }
}

// GET method to retrieve chat history (optional)
export async function GET(req: NextRequest) {
  try {
    const conversationId = req.nextUrl.searchParams.get("conversation_id");

    // For now, return empty history
    // In the future, you can implement conversation storage in Supabase
    return NextResponse.json({
      messages: [],
      conversation_id: conversationId,
    }, { headers: corsHeaders });
  } catch (error) {
    console.error("âŒ Error fetching chat history:", error);
    return NextResponse.json(
      { error: "internal server error" },
      { status: 500, headers: corsHeaders }
    );
  }
}
